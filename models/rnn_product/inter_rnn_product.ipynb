{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903246be",
   "metadata": {},
   "source": [
    "## Prepare Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a33499fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18dc6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_1d(array, max_len):\n",
    "    array = list(array)[:max_len]\n",
    "    length = len(array)\n",
    "    padded = array + [0]*(max_len - len(array))\n",
    "    return padded, length\n",
    "\n",
    "\n",
    "def make_word_idx(product_names):\n",
    "    words = [word for name in product_names for word in name.split()]\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    max_id = 1\n",
    "    word_idx = {}\n",
    "    for word, count in word_counts.items():\n",
    "        if count < 10:\n",
    "            word_idx[word] = 0\n",
    "        else:\n",
    "            word_idx[word] = max_id\n",
    "            max_id += 1\n",
    "\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def encode_text(text, word_idx):\n",
    "    return ' '.join([str(word_idx[i]) for i in text.split()]) if text else '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1bddabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding debugging\n",
    "product_data = pd.read_csv('../../data/processed/product_data.csv')\n",
    "# Remove floats\n",
    "product_data = product_data.loc[product_data['product_name'].apply(lambda x: isinstance(x, str)),:]\n",
    "product_data = product_data.loc[product_data['is_ordered_history'].apply(lambda x: isinstance(x, str)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f93d2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 116178, 1: 12416, -1: 9209}\n"
     ]
    }
   ],
   "source": [
    "# Check Data Shape\n",
    "product_data.shape\n",
    "# Check Label type\n",
    "label_col = product_data['label']\n",
    "label_set = set(label_col)\n",
    "label_count = {i: sum(label_col == i) for i in label_set}\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b495f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Embedding\n",
    "product_data['product_name'] = product_data['product_name'].map(lambda x: x.lower() if type(x)==str else 0)\n",
    "\n",
    "product_df = pd.read_csv('../../data/raw/products.csv')\n",
    "product_df['product_name'] = product_df['product_name'].map(lambda x: x.lower())\n",
    "\n",
    "word_idx = make_word_idx(product_df['product_name'].tolist())\n",
    "product_data['product_name_encoded'] = product_data['product_name'].map(lambda x: encode_text(x, word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ca97a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'str'>: 137803}\n"
     ]
    }
   ],
   "source": [
    "# Check the property of list product_name\n",
    "mixlist = product_data['product_name']\n",
    "mixlist_type = [type(s) for s in mixlist]\n",
    "mixlist_type_dic = {t: mixlist_type.count(t) for t in set(mixlist_type)}\n",
    "print(mixlist_type_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccf122ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(product_data)\n",
    "\n",
    "user_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "product_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "aisle_id = np.zeros(shape=[num_rows], dtype=np.int16)\n",
    "department_id = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "eval_set = np.zeros(shape=[num_rows], dtype='S5')\n",
    "label = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "\n",
    "is_ordered_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "index_in_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_dow_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_hour_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "days_since_prior_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "reorder_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_number_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "product_name = np.zeros(shape=[num_rows, 30], dtype=np.int32)\n",
    "product_name_length = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "history_length = np.zeros(shape=[num_rows], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54451000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137803, 16) (137803,)\n"
     ]
    }
   ],
   "source": [
    "# Check the length of lists\n",
    "print(product_data.shape, user_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c99ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 137803\n",
      "20000 137803\n",
      "30000 137803\n",
      "40000 137803\n",
      "50000 137803\n",
      "60000 137803\n",
      "70000 137803\n",
      "80000 137803\n",
      "90000 137803\n",
      "100000 137803\n",
      "110000 137803\n",
      "120000 137803\n",
      "130000 137803\n"
     ]
    }
   ],
   "source": [
    "for i, row in product_data.iterrows():\n",
    "    # Index Error Check: False\n",
    "    # i = i - 1\n",
    "    if i % 10000 == 0:\n",
    "        print(i, num_rows)\n",
    "    \n",
    "    # Avoid over indexing\n",
    "    if i == num_rows:\n",
    "        break\n",
    "\n",
    "    user_id[i] = row['user_id']\n",
    "    product_id[i] = row['product_id']\n",
    "    aisle_id[i] = row['aisle_id']\n",
    "    department_id[i] = row['department_id']\n",
    "    eval_set[i] = row['eval_set']\n",
    "    label[i] = row['label']\n",
    "\n",
    "    is_ordered_history[i, :], history_length[i] = pad_1d(list(map(int, row['is_ordered_history'].split())), 100)\n",
    "    index_in_order_history[i, :], _ = pad_1d(list(map(int, row['index_in_order_history'].split())), 100)\n",
    "    order_dow_history[i, :], _ = pad_1d(list(map(int, row['order_dow_history'].split())), 100)\n",
    "    order_hour_history[i, :], _ = pad_1d(list(map(int, row['order_hour_history'].split())), 100)\n",
    "    days_since_prior_order_history[i, :], _ = pad_1d(list(map(int, row['days_since_prior_order_history'].split())), 100)\n",
    "    order_size_history[i, :], _ = pad_1d(list(map(int, row['order_size_history'].split())), 100)\n",
    "    reorder_size_history[i, :], _ = pad_1d(list(map(int, row['reorder_size_history'].split())), 100)\n",
    "    order_number_history[i, :], _ = pad_1d(list(map(int, row['order_number_history'].split())), 100)\n",
    "    product_name[i, :], product_name_length[i] = pad_1d(list(map(int, row['product_name_encoded'].split())), 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "97fbce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137803,) (137803, 100) (137803, 100)\n"
     ]
    }
   ],
   "source": [
    "# Length check\n",
    "print(user_id.shape, is_ordered_history.shape, order_dow_history.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49bc40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "np.save('data/user_id.npy', user_id)\n",
    "np.save('data/product_id.npy', product_id)\n",
    "np.save('data/aisle_id.npy', aisle_id)\n",
    "np.save('data/department_id.npy', department_id)\n",
    "np.save('data/eval_set.npy', eval_set)\n",
    "np.save('data/label.npy', label)\n",
    "\n",
    "np.save('data/is_ordered_history.npy', is_ordered_history)\n",
    "np.save('data/index_in_order_history.npy', index_in_order_history)\n",
    "np.save('data/order_dow_history.npy', order_dow_history)\n",
    "np.save('data/order_hour_history.npy', order_hour_history)\n",
    "np.save('data/days_since_prior_order_history.npy', days_since_prior_order_history)\n",
    "np.save('data/order_size_history.npy', order_size_history)\n",
    "np.save('data/reorder_size_history.npy', reorder_size_history)\n",
    "np.save('data/order_number_history.npy', order_number_history)\n",
    "np.save('data/product_name.npy', product_name)\n",
    "np.save('data/product_name_length.npy', product_name_length)\n",
    "np.save('data/history_length.npy', history_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4ca9d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f58b03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8457653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae5b6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized Function\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from data_frame import DataFrame\n",
    "from tf_utils import lstm_layer, time_distributed_dense_layer, dense_layer, sequence_log_loss, wavenet\n",
    "from tf_base_model import TFBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2295b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional packages for python 2 functions\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dbd6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print(self.test_df.shapes())\n",
    "        print(\"loaded data\")\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        # Output set information\n",
    "        print('train size', len(self.train_df))\n",
    "        print('validation size', len(self.val_df))\n",
    "        print('test size', len(self.test_df))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            batch['is_none'] = batch['product_id'] == 0\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ab93fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(TFBaseModel):\n",
    "\n",
    "    def __init__(self, lstm_size, dilations, filter_widths, skip_channels, residual_channels, **kwargs):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        super(rnn, self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        preds = self.calculate_outputs(x)\n",
    "        loss = sequence_log_loss(self.next_is_ordered, preds, self.history_length, 100)\n",
    "        return loss\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.user_id = tf.placeholder(tf.int32, [None])\n",
    "        self.product_id = tf.placeholder(tf.int32, [None])\n",
    "        self.aisle_id = tf.placeholder(tf.int32, [None])\n",
    "        self.department_id = tf.placeholder(tf.int32, [None])\n",
    "        self.is_none = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.is_ordered_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.index_in_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_dow_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_hour_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.days_since_prior_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.reorder_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_number_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.product_name = tf.placeholder(tf.int32, [None, 30])\n",
    "        self.product_name_length = tf.placeholder(tf.int32, [None])\n",
    "        self.next_is_ordered = tf.placeholder(tf.int32, [None, 100])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # product data\n",
    "        product_embeddings = tf.get_variable(\n",
    "            name='product_embeddings',\n",
    "            shape=[50000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        aisle_embeddings = tf.get_variable(\n",
    "            name='aisle_embeddings',\n",
    "            shape=[250, 50],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        department_embeddings = tf.get_variable(\n",
    "            name='department_embeddings',\n",
    "            shape=[50, 10],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        product_names = tf.one_hot(self.product_name, 2532)\n",
    "        product_names = tf.reduce_max(product_names, 1)\n",
    "        product_names = dense_layer(product_names, 100, activation=tf.nn.relu)\n",
    "\n",
    "        is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "\n",
    "        x_product = tf.concat([\n",
    "            tf.nn.embedding_lookup(product_embeddings, self.product_id),\n",
    "            tf.nn.embedding_lookup(aisle_embeddings, self.aisle_id),\n",
    "            tf.nn.embedding_lookup(department_embeddings, self.department_id),\n",
    "            is_none,\n",
    "            product_names\n",
    "        ], axis=1)\n",
    "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "\n",
    "        # user data\n",
    "        user_embeddings = tf.get_variable(\n",
    "            name='user_embeddings',\n",
    "            shape=[207000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        x_user = tf.nn.embedding_lookup(user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "\n",
    "        # sequence data\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history,\n",
    "            index_in_order_history,\n",
    "            order_dow_history,\n",
    "            order_hour_history,\n",
    "            days_since_prior_order_history,\n",
    "            order_size_history,\n",
    "            reorder_size_history,\n",
    "            order_number_history,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "        x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calculate_outputs(self, x):\n",
    "        h = lstm_layer(x, self.history_length, self.lstm_size)\n",
    "        c = wavenet(x, self.dilations, self.filter_widths, self.skip_channels, self.residual_channels)\n",
    "        h = tf.concat([h, c, x], axis=2)\n",
    "\n",
    "        self.h_final = time_distributed_dense_layer(h, 50, activation=tf.nn.relu, scope='dense-1')\n",
    "        y_hat = time_distributed_dense_layer(self.h_final, 1, activation=tf.nn.sigmoid, scope='dense-2')\n",
    "        y_hat = tf.squeeze(y_hat, 2)\n",
    "\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), tf.maximum(self.history_length - 1, 0)], axis=1)\n",
    "        self.final_states = tf.gather_nd(self.h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "        }\n",
    "\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                               (137803,)\n",
      "product_id                            (137803,)\n",
      "aisle_id                              (137803,)\n",
      "department_id                         (137803,)\n",
      "is_ordered_history                (137803, 100)\n",
      "index_in_order_history            (137803, 100)\n",
      "order_dow_history                 (137803, 100)\n",
      "order_hour_history                (137803, 100)\n",
      "days_since_prior_order_history    (137803, 100)\n",
      "order_size_history                (137803, 100)\n",
      "reorder_size_history              (137803, 100)\n",
      "order_number_history              (137803, 100)\n",
      "history_length                        (137803,)\n",
      "product_name                       (137803, 30)\n",
      "product_name_length                   (137803,)\n",
      "eval_set                              (137803,)\n",
      "label                                 (137803,)\n",
      "dtype: object\n",
      "loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new run with parameters:\n",
      "{'batch_size': 128,\n",
      " 'checkpoint_dir': './checkpoints',\n",
      " 'dilations': [1, 2, 4, 8, 16, 32],\n",
      " 'early_stopping_steps': 100,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'filter_widths': [2, 2, 2, 2, 2, 2],\n",
      " 'grad_clip': 5,\n",
      " 'keep_prob_scalar': 0.5,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_dir': './logs',\n",
      " 'log_interval': 20,\n",
      " 'loss_averaging_window': 100,\n",
      " 'lstm_size': 100,\n",
      " 'min_steps_to_checkpoint': 200,\n",
      " 'num_restarts': 2,\n",
      " 'num_training_steps': 2000,\n",
      " 'num_validation_batches': 4,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './predictions',\n",
      " 'reader': <__main__.DataReader object at 0x2add840784a8>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'residual_channels': 128,\n",
      " 'skip_channels': 64,\n",
      " 'warm_start_init_step': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 124022\n",
      "validation size 13781\n",
      "test size 137803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all parameters:\n",
      "[('product_embeddings:0', [50000, 100]),\n",
      " ('aisle_embeddings:0', [250, 50]),\n",
      " ('department_embeddings:0', [50, 10]),\n",
      " ('dense-layer/weights:0', [2532, 100]),\n",
      " ('dense-layer/biases:0', [100]),\n",
      " ('user_embeddings:0', [207000, 100]),\n",
      " ('lstm-layer/rnn/lstm_cell/kernel:0', [765, 400]),\n",
      " ('lstm-layer/rnn/lstm_cell/bias:0', [400]),\n",
      " ('wavenet/x-proj/weights:0', [665, 128]),\n",
      " ('wavenet/x-proj/biases:0', [128]),\n",
      " ('wavenet/cnn-0/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-0/biases:0', [256]),\n",
      " ('wavenet/cnn-0-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-0-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-1/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-1/biases:0', [256]),\n",
      " ('wavenet/cnn-1-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-1-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-2/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-2/biases:0', [256]),\n",
      " ('wavenet/cnn-2-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-2-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-3/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-3/biases:0', [256]),\n",
      " ('wavenet/cnn-3-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-3-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-4/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-4/biases:0', [256]),\n",
      " ('wavenet/cnn-4-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-4-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-5/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-5/biases:0', [256]),\n",
      " ('wavenet/cnn-5-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-5-proj/biases:0', [192]),\n",
      " ('dense-1/weights:0', [1149, 50]),\n",
      " ('dense-1/biases:0', [50]),\n",
      " ('dense-2/weights:0', [50, 1]),\n",
      " ('dense-2/biases:0', [1]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('product_embeddings/Adam:0', [50000, 100]),\n",
      " ('product_embeddings/Adam_1:0', [50000, 100]),\n",
      " ('aisle_embeddings/Adam:0', [250, 50]),\n",
      " ('aisle_embeddings/Adam_1:0', [250, 50]),\n",
      " ('department_embeddings/Adam:0', [50, 10]),\n",
      " ('department_embeddings/Adam_1:0', [50, 10]),\n",
      " ('dense-layer/weights/Adam:0', [2532, 100]),\n",
      " ('dense-layer/weights/Adam_1:0', [2532, 100]),\n",
      " ('dense-layer/biases/Adam:0', [100]),\n",
      " ('dense-layer/biases/Adam_1:0', [100]),\n",
      " ('user_embeddings/Adam:0', [207000, 100]),\n",
      " ('user_embeddings/Adam_1:0', [207000, 100]),\n",
      " ('lstm-layer/rnn/lstm_cell/kernel/Adam:0', [765, 400]),\n",
      " ('lstm-layer/rnn/lstm_cell/kernel/Adam_1:0', [765, 400]),\n",
      " ('lstm-layer/rnn/lstm_cell/bias/Adam:0', [400]),\n",
      " ('lstm-layer/rnn/lstm_cell/bias/Adam_1:0', [400]),\n",
      " ('wavenet/x-proj/weights/Adam:0', [665, 128]),\n",
      " ('wavenet/x-proj/weights/Adam_1:0', [665, 128]),\n",
      " ('wavenet/x-proj/biases/Adam:0', [128]),\n",
      " ('wavenet/x-proj/biases/Adam_1:0', [128]),\n",
      " ('wavenet/cnn-0/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-0/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-0/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-0/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-0-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-0-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-0-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-0-proj/biases/Adam_1:0', [192]),\n",
      " ('wavenet/cnn-1/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-1/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-1/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-1/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-1-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-1-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-1-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-1-proj/biases/Adam_1:0', [192]),\n",
      " ('wavenet/cnn-2/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-2/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-2/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-2/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-2-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-2-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-2-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-2-proj/biases/Adam_1:0', [192]),\n",
      " ('wavenet/cnn-3/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-3/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-3/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-3/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-3-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-3-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-3-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-3-proj/biases/Adam_1:0', [192]),\n",
      " ('wavenet/cnn-4/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-4/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-4/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-4/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-4-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-4-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-4-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-4-proj/biases/Adam_1:0', [192]),\n",
      " ('wavenet/cnn-5/weights/Adam:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-5/weights/Adam_1:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-5/biases/Adam:0', [256]),\n",
      " ('wavenet/cnn-5/biases/Adam_1:0', [256]),\n",
      " ('wavenet/cnn-5-proj/weights/Adam:0', [128, 192]),\n",
      " ('wavenet/cnn-5-proj/weights/Adam_1:0', [128, 192]),\n",
      " ('wavenet/cnn-5-proj/biases/Adam:0', [192]),\n",
      " ('wavenet/cnn-5-proj/biases/Adam_1:0', [192]),\n",
      " ('dense-1/weights/Adam:0', [1149, 50]),\n",
      " ('dense-1/weights/Adam_1:0', [1149, 50]),\n",
      " ('dense-1/biases/Adam:0', [50]),\n",
      " ('dense-1/biases/Adam_1:0', [50]),\n",
      " ('dense-2/weights/Adam:0', [50, 1]),\n",
      " ('dense-2/weights/Adam_1:0', [50, 1]),\n",
      " ('dense-2/biases/Adam:0', [1]),\n",
      " ('dense-2/biases/Adam_1:0', [1])]\n",
      "trainable parameters:\n",
      "[('product_embeddings:0', [50000, 100]),\n",
      " ('aisle_embeddings:0', [250, 50]),\n",
      " ('department_embeddings:0', [50, 10]),\n",
      " ('dense-layer/weights:0', [2532, 100]),\n",
      " ('dense-layer/biases:0', [100]),\n",
      " ('user_embeddings:0', [207000, 100]),\n",
      " ('lstm-layer/rnn/lstm_cell/kernel:0', [765, 400]),\n",
      " ('lstm-layer/rnn/lstm_cell/bias:0', [400]),\n",
      " ('wavenet/x-proj/weights:0', [665, 128]),\n",
      " ('wavenet/x-proj/biases:0', [128]),\n",
      " ('wavenet/cnn-0/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-0/biases:0', [256]),\n",
      " ('wavenet/cnn-0-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-0-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-1/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-1/biases:0', [256]),\n",
      " ('wavenet/cnn-1-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-1-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-2/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-2/biases:0', [256]),\n",
      " ('wavenet/cnn-2-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-2-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-3/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-3/biases:0', [256]),\n",
      " ('wavenet/cnn-3-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-3-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-4/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-4/biases:0', [256]),\n",
      " ('wavenet/cnn-4-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-4-proj/biases:0', [192]),\n",
      " ('wavenet/cnn-5/weights:0', [2, 128, 256]),\n",
      " ('wavenet/cnn-5/biases:0', [256]),\n",
      " ('wavenet/cnn-5-proj/weights:0', [128, 192]),\n",
      " ('wavenet/cnn-5-proj/biases:0', [192]),\n",
      " ('dense-1/weights:0', [1149, 50]),\n",
      " ('dense-1/biases:0', [50]),\n",
      " ('dense-2/weights:0', [50, 1]),\n",
      " ('dense-2/biases:0', [1])]\n",
      "trainable parameter count:\n",
      "26958859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.50389457       [[val]]     loss: 0.50544643       \n",
      "[[step       20]]     [[train]]     loss: 0.0440548        [[val]]     loss: 0.05221456       \n",
      "[[step       40]]     [[train]]     loss: 0.03051687       [[val]]     loss: 0.03462561       \n",
      "[[step       60]]     [[train]]     loss: 0.02406596       [[val]]     loss: 0.02746421       \n",
      "[[step       80]]     [[train]]     loss: 0.02041885       [[val]]     loss: 0.02338304       \n",
      "[[step      100]]     [[train]]     loss: 0.01258761       [[val]]     loss: 0.01535527       \n",
      "[[step      120]]     [[train]]     loss: 0.00890037       [[val]]     loss: 0.01006605       \n",
      "[[step      140]]     [[train]]     loss: 0.00578102       [[val]]     loss: 0.00698902       \n",
      "[[step      160]]     [[train]]     loss: 0.00362739       [[val]]     loss: 0.00444908       \n"
     ]
    }
   ],
   "source": [
    "base_dir = './'\n",
    "\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, 'data'))\n",
    "\n",
    "nn = rnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "    optimizer='adam',\n",
    "    learning_rate=.001,\n",
    "    lstm_size=100,\n",
    "    dilations=[2**i for i in range(6)],\n",
    "    filter_widths=[2]*6,\n",
    "    skip_channels=64,\n",
    "    residual_channels=128,\n",
    "    batch_size=128,\n",
    "    num_training_steps=2000,\n",
    "    early_stopping_steps=100,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    keep_prob=0.5,\n",
    "    enable_parameter_averaging=False,\n",
    "    num_restarts=2,\n",
    "    min_steps_to_checkpoint=200,\n",
    "    log_interval=20,\n",
    "    num_validation_batches=4,\n",
    ")\n",
    "nn.fit() # Training finished, start prediction\n",
    "nn.restore()\n",
    "nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac97bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
