{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903246be",
   "metadata": {},
   "source": [
    "## Prepare Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33499fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18dc6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_1d(array, max_len):\n",
    "    array = list(array)[:max_len]\n",
    "    length = len(array)\n",
    "    padded = array + [0]*(max_len - len(array))\n",
    "    return padded, length\n",
    "\n",
    "\n",
    "def make_word_idx(product_names):\n",
    "    words = [word for name in product_names for word in name.split()]\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    max_id = 1\n",
    "    word_idx = {}\n",
    "    for word, count in word_counts.items():\n",
    "        if count < 10:\n",
    "            word_idx[word] = 0\n",
    "        else:\n",
    "            word_idx[word] = max_id\n",
    "            max_id += 1\n",
    "\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def encode_text(text, word_idx):\n",
    "    return ' '.join([str(word_idx[i]) for i in text.split()]) if text else '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bddabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (4,5,6,7,8,9,10,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Embedding debugging\n",
    "product_data = pd.read_csv('../../data/processed/product_data.csv')\n",
    "# Remove floats\n",
    "product_data = product_data.loc[product_data['product_name'].apply(lambda x: isinstance(x, str)),:]\n",
    "product_data = product_data.loc[product_data['is_ordered_history'].apply(lambda x: isinstance(x, str)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93d2b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 119128, 1: 13286, -1: 272304}\n"
     ]
    }
   ],
   "source": [
    "# Check Data Shape\n",
    "product_data.shape\n",
    "# Check Label type\n",
    "label_col = product_data['label']\n",
    "label_set = set(label_col)\n",
    "label_count = {i: sum(label_col == i) for i in label_set}\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2eeb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Embedding\n",
    "product_data['product_name'] = product_data['product_name'].map(lambda x: x.lower() if type(x)==str else 0)\n",
    "\n",
    "product_df = pd.read_csv('../../data/raw/products.csv')\n",
    "product_df['product_name'] = product_df['product_name'].map(lambda x: x.lower())\n",
    "\n",
    "word_idx = make_word_idx(product_df['product_name'].tolist())\n",
    "product_data['product_name_encoded'] = product_data['product_name'].map(lambda x: encode_text(x, word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca97a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<class 'str'>: 404718}\n"
     ]
    }
   ],
   "source": [
    "# Check the property of list product_name\n",
    "mixlist = product_data['product_name']\n",
    "mixlist_type = [type(s) for s in mixlist]\n",
    "mixlist_type_dic = {t: mixlist_type.count(t) for t in set(mixlist_type)}\n",
    "print(mixlist_type_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf122ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(product_data)\n",
    "\n",
    "user_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "product_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "aisle_id = np.zeros(shape=[num_rows], dtype=np.int16)\n",
    "department_id = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "eval_set = np.zeros(shape=[num_rows], dtype='S5')\n",
    "label = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "\n",
    "is_ordered_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "index_in_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_dow_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_hour_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "days_since_prior_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "reorder_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_number_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "product_name = np.zeros(shape=[num_rows, 30], dtype=np.int32)\n",
    "product_name_length = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "history_length = np.zeros(shape=[num_rows], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54451000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404718, 16) (404718,)\n"
     ]
    }
   ],
   "source": [
    "# Check the length of lists\n",
    "print(product_data.shape, user_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c99ce37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 404718\n",
      "10000 404718\n",
      "20000 404718\n",
      "30000 404718\n",
      "40000 404718\n",
      "50000 404718\n",
      "60000 404718\n",
      "70000 404718\n",
      "80000 404718\n",
      "90000 404718\n",
      "100000 404718\n",
      "110000 404718\n",
      "120000 404718\n",
      "130000 404718\n",
      "140000 404718\n",
      "150000 404718\n",
      "160000 404718\n",
      "170000 404718\n",
      "180000 404718\n",
      "190000 404718\n",
      "200000 404718\n",
      "220000 404718\n",
      "230000 404718\n",
      "240000 404718\n",
      "250000 404718\n",
      "260000 404718\n",
      "270000 404718\n",
      "280000 404718\n",
      "290000 404718\n",
      "300000 404718\n",
      "310000 404718\n",
      "320000 404718\n",
      "350000 404718\n",
      "360000 404718\n",
      "370000 404718\n",
      "390000 404718\n",
      "400000 404718\n"
     ]
    }
   ],
   "source": [
    "for i, row in product_data.iterrows():\n",
    "    # Index Error Check: False\n",
    "    # i = i - 1\n",
    "    if i % 10000 == 0:\n",
    "        print(i, num_rows)\n",
    "    \n",
    "    # Avoid over indexing\n",
    "    if i == num_rows:\n",
    "        break\n",
    "\n",
    "    user_id[i] = row['user_id']\n",
    "    product_id[i] = row['product_id']\n",
    "    aisle_id[i] = row['aisle_id']\n",
    "    department_id[i] = row['department_id']\n",
    "    eval_set[i] = row['eval_set']\n",
    "    label[i] = row['label']\n",
    "\n",
    "    is_ordered_history[i, :], history_length[i] = pad_1d(list(map(int, row['is_ordered_history'].split())), 100)\n",
    "    index_in_order_history[i, :], _ = pad_1d(list(map(int, row['index_in_order_history'].split())), 100)\n",
    "    order_dow_history[i, :], _ = pad_1d(list(map(int, row['order_dow_history'].split())), 100)\n",
    "    order_hour_history[i, :], _ = pad_1d(list(map(int, row['order_hour_history'].split())), 100)\n",
    "    days_since_prior_order_history[i, :], _ = pad_1d(list(map(int, row['days_since_prior_order_history'].split())), 100)\n",
    "    order_size_history[i, :], _ = pad_1d(list(map(int, row['order_size_history'].split())), 100)\n",
    "    reorder_size_history[i, :], _ = pad_1d(list(map(int, row['reorder_size_history'].split())), 100)\n",
    "    order_number_history[i, :], _ = pad_1d(list(map(int, row['order_number_history'].split())), 100)\n",
    "    product_name[i, :], product_name_length[i] = pad_1d(list(map(int, row['product_name_encoded'].split())), 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97fbce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404718,) (404718, 100) (404718, 100)\n"
     ]
    }
   ],
   "source": [
    "# Length check\n",
    "print(user_id.shape, is_ordered_history.shape, order_dow_history.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49bc40fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "np.save('data/user_id.npy', user_id)\n",
    "np.save('data/product_id.npy', product_id)\n",
    "np.save('data/aisle_id.npy', aisle_id)\n",
    "np.save('data/department_id.npy', department_id)\n",
    "np.save('data/eval_set.npy', eval_set)\n",
    "np.save('data/label.npy', label)\n",
    "\n",
    "np.save('data/is_ordered_history.npy', is_ordered_history)\n",
    "np.save('data/index_in_order_history.npy', index_in_order_history)\n",
    "np.save('data/order_dow_history.npy', order_dow_history)\n",
    "np.save('data/order_hour_history.npy', order_hour_history)\n",
    "np.save('data/days_since_prior_order_history.npy', days_since_prior_order_history)\n",
    "np.save('data/order_size_history.npy', order_size_history)\n",
    "np.save('data/reorder_size_history.npy', reorder_size_history)\n",
    "np.save('data/order_number_history.npy', order_number_history)\n",
    "np.save('data/product_name.npy', product_name)\n",
    "np.save('data/product_name_length.npy', product_name_length)\n",
    "np.save('data/history_length.npy', history_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b4ca9d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58b03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8457653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5b6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized Function\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from data_frame import DataFrame\n",
    "from tf_utils import lstm_layer, time_distributed_dense_layer, dense_layer, sequence_log_loss, wavenet, log_loss\n",
    "from tf_base_model import TFBaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2295b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional packages for python 2 functions\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dbd6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print(self.test_df.shapes())\n",
    "        print(\"loaded data\")\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        # Output set information\n",
    "        print('train size', len(self.train_df))\n",
    "        print('validation size', len(self.val_df))\n",
    "        print('test size', len(self.test_df))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            batch['is_none'] = batch['product_id'] == 0\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab93fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn(TFBaseModel):\n",
    "\n",
    "    def __init__(self, lstm_size=300, **kwargs):\n",
    "        self.lstm_size = lstm_size\n",
    "        super(rnn, self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        return self.calculate_outputs(x)\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.user_id = tf.placeholder(tf.int32, [None])\n",
    "        self.product_id = tf.placeholder(tf.int32, [None])\n",
    "        self.aisle_id = tf.placeholder(tf.int32, [None])\n",
    "        self.department_id = tf.placeholder(tf.int32, [None])\n",
    "        self.is_none = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.is_ordered_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.index_in_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_dow_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_hour_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.days_since_prior_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.reorder_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_number_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.product_name = tf.placeholder(tf.int32, [None, 30])\n",
    "        self.product_name_length = tf.placeholder(tf.int32, [None])\n",
    "        self.next_is_ordered = tf.placeholder(tf.int32, [None, 100])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # product data\n",
    "        product_embeddings = tf.get_variable(\n",
    "            name='product_embeddings',\n",
    "            shape=[50000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        aisle_embeddings = tf.get_variable(\n",
    "            name='aisle_embeddings',\n",
    "            shape=[250, 50],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        department_embeddings = tf.get_variable(\n",
    "            name='department_embeddings',\n",
    "            shape=[50, 10],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        product_names = tf.one_hot(self.product_name, 2532)\n",
    "        product_names = tf.reduce_max(product_names, 1)\n",
    "        product_names = dense_layer(product_names, 100, activation=tf.nn.relu)\n",
    "\n",
    "        is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "\n",
    "        x_product = tf.concat([\n",
    "            tf.nn.embedding_lookup(product_embeddings, self.product_id),\n",
    "            tf.nn.embedding_lookup(aisle_embeddings, self.aisle_id),\n",
    "            tf.nn.embedding_lookup(department_embeddings, self.department_id),\n",
    "            is_none,\n",
    "            product_names\n",
    "        ], axis=1)\n",
    "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "\n",
    "        # user data\n",
    "        user_embeddings = tf.get_variable(\n",
    "            name='user_embeddings',\n",
    "            shape=[207000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        x_user = tf.nn.embedding_lookup(user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "\n",
    "        # sequence data\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history,\n",
    "            index_in_order_history,\n",
    "            order_dow_history,\n",
    "            order_hour_history,\n",
    "            days_since_prior_order_history,\n",
    "            order_size_history,\n",
    "            reorder_size_history,\n",
    "            order_number_history,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "        x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calculate_outputs(self, x):\n",
    "        # An LSTM layer is applied to the input 'x'. The output 'h' is a sequence of hidden states.\n",
    "        h = lstm_layer(x, self.history_length, self.lstm_size, scope='lstm-1')\n",
    "        \n",
    "        # The output of the LSTM layer 'h' is concatenated with the original input 'x'. \n",
    "        # This is a common technique in sequence-to-sequence models, allowing the model to have access to the original input in later layers.\n",
    "        h = tf.concat([h, x], axis=2)\n",
    "        \n",
    "        # The concatenated output is passed through a dense layer with ReLU activation function. \n",
    "        # This serves to transform the data into a more useful form for the final prediction.\n",
    "        h_final = time_distributed_dense_layer(h, 50, activation=tf.nn.relu, scope='dense-1')\n",
    "\n",
    "        # The number of components for the mixture model is set to 1.\n",
    "        n_components = 1\n",
    "        \n",
    "        # Another dense layer is applied to 'h_final' without an activation function. \n",
    "        # The output 2-d 'params' is then split into two parts: 'ps' and 'mixing_coefs'.\n",
    "        params = time_distributed_dense_layer(h_final, n_components*2, scope='dense-2', activation=None)\n",
    "        ps, mixing_coefs = tf.split(params, 2, axis=2)\n",
    "\n",
    "        # The 'mixing_coefs' are passed through a softmax function, which normalizes them to sum to 1. \n",
    "        # This is a common operation when the coefficients represent a mixture of components.\n",
    "        # However, the author notes that this is implemented incorrectly, likely because there is only one score per sequence\n",
    "        mixing_coefs = tf.nn.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, 2, keep_dims=True))\n",
    "        \n",
    "        # The 'ps' are passed through a sigmoid function, which squashes their values to the range [0, 1]. \n",
    "        # This is a common operation when the output represents a probability.\n",
    "        ps = tf.nn.sigmoid(ps)\n",
    "\n",
    "        # The labels are replicated to match the shape of 'ps' and 'mixing_coefs'.\n",
    "        labels = tf.tile(tf.expand_dims(self.next_is_ordered, 2), (1, 1, n_components))\n",
    "        \n",
    "        # The losses are calculated as the sum of the product of 'mixing_coefs' and the log loss between 'labels' and 'ps', averaged over the sequence length.\n",
    "        losses = tf.reduce_sum(mixing_coefs*log_loss(labels, ps), axis=2)\n",
    "        sequence_mask = tf.cast(tf.sequence_mask(self.history_length, maxlen=100), tf.float32)\n",
    "        avg_loss = tf.reduce_sum(losses*sequence_mask) / tf.cast(tf.reduce_sum(self.history_length), tf.float32)\n",
    "\n",
    "        # The final states are selected from 'h_final' and stored in 'self.final_states'.\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        self.final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(ps, final_temporal_idx) # Add prediction part\n",
    "\n",
    "        # The prediction tensors are stored in 'self.prediction_tensors'.\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "        }\n",
    "\n",
    "        # The function returns the average loss.\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164d7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id                               (404718,)\n",
      "product_id                            (404718,)\n",
      "aisle_id                              (404718,)\n",
      "department_id                         (404718,)\n",
      "is_ordered_history                (404718, 100)\n",
      "index_in_order_history            (404718, 100)\n",
      "order_dow_history                 (404718, 100)\n",
      "order_hour_history                (404718, 100)\n",
      "days_since_prior_order_history    (404718, 100)\n",
      "order_size_history                (404718, 100)\n",
      "reorder_size_history              (404718, 100)\n",
      "order_number_history              (404718, 100)\n",
      "history_length                        (404718,)\n",
      "product_name                       (404718, 30)\n",
      "product_name_length                   (404718,)\n",
      "eval_set                              (404718,)\n",
      "label                                 (404718,)\n",
      "dtype: object\n",
      "loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new run with parameters:\n",
      "{'batch_size': 128,\n",
      " 'checkpoint_dir': './checkpoints_bmm',\n",
      " 'early_stopping_steps': 100,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'grad_clip': 5,\n",
      " 'keep_prob_scalar': 0.5,\n",
      " 'learning_rate': 0.001,\n",
      " 'log_dir': './logs_bmm',\n",
      " 'log_interval': 20,\n",
      " 'loss_averaging_window': 100,\n",
      " 'lstm_size': 300,\n",
      " 'min_steps_to_checkpoint': 200,\n",
      " 'num_restarts': 2,\n",
      " 'num_training_steps': 800,\n",
      " 'num_validation_batches': 4,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './predictions_bmm',\n",
      " 'reader': <__main__.DataReader object at 0x2b4ee9af1e48>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'warm_start_init_step': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 364246\n",
      "validation size 40472\n",
      "test size 404718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all parameters:\n",
      "[('product_embeddings:0', [50000, 300]),\n",
      " ('aisle_embeddings:0', [250, 50]),\n",
      " ('department_embeddings:0', [50, 10]),\n",
      " ('dense-layer/weights:0', [2532, 100]),\n",
      " ('dense-layer/biases:0', [100]),\n",
      " ('user_embeddings:0', [207000, 300]),\n",
      " ('lstm-1/rnn/lstm_cell/kernel:0', [1365, 1200]),\n",
      " ('lstm-1/rnn/lstm_cell/bias:0', [1200]),\n",
      " ('dense-1/weights:0', [1365, 50]),\n",
      " ('dense-1/biases:0', [50]),\n",
      " ('dense-2/weights:0', [50, 2]),\n",
      " ('dense-2/biases:0', [2]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('product_embeddings/Adam:0', [50000, 300]),\n",
      " ('product_embeddings/Adam_1:0', [50000, 300]),\n",
      " ('aisle_embeddings/Adam:0', [250, 50]),\n",
      " ('aisle_embeddings/Adam_1:0', [250, 50]),\n",
      " ('department_embeddings/Adam:0', [50, 10]),\n",
      " ('department_embeddings/Adam_1:0', [50, 10]),\n",
      " ('dense-layer/weights/Adam:0', [2532, 100]),\n",
      " ('dense-layer/weights/Adam_1:0', [2532, 100]),\n",
      " ('dense-layer/biases/Adam:0', [100]),\n",
      " ('dense-layer/biases/Adam_1:0', [100]),\n",
      " ('user_embeddings/Adam:0', [207000, 300]),\n",
      " ('user_embeddings/Adam_1:0', [207000, 300]),\n",
      " ('lstm-1/rnn/lstm_cell/kernel/Adam:0', [1365, 1200]),\n",
      " ('lstm-1/rnn/lstm_cell/kernel/Adam_1:0', [1365, 1200]),\n",
      " ('lstm-1/rnn/lstm_cell/bias/Adam:0', [1200]),\n",
      " ('lstm-1/rnn/lstm_cell/bias/Adam_1:0', [1200]),\n",
      " ('dense-1/weights/Adam:0', [1365, 50]),\n",
      " ('dense-1/weights/Adam_1:0', [1365, 50]),\n",
      " ('dense-1/biases/Adam:0', [50]),\n",
      " ('dense-1/biases/Adam_1:0', [50]),\n",
      " ('dense-2/weights/Adam:0', [50, 2]),\n",
      " ('dense-2/weights/Adam_1:0', [50, 2]),\n",
      " ('dense-2/biases/Adam:0', [2]),\n",
      " ('dense-2/biases/Adam_1:0', [2])]\n",
      "trainable parameters:\n",
      "[('product_embeddings:0', [50000, 300]),\n",
      " ('aisle_embeddings:0', [250, 50]),\n",
      " ('department_embeddings:0', [50, 10]),\n",
      " ('dense-layer/weights:0', [2532, 100]),\n",
      " ('dense-layer/biases:0', [100]),\n",
      " ('user_embeddings:0', [207000, 300]),\n",
      " ('lstm-1/rnn/lstm_cell/kernel:0', [1365, 1200]),\n",
      " ('lstm-1/rnn/lstm_cell/bias:0', [1200]),\n",
      " ('dense-1/weights:0', [1365, 50]),\n",
      " ('dense-1/biases:0', [50]),\n",
      " ('dense-2/weights:0', [50, 2]),\n",
      " ('dense-2/biases:0', [2])]\n",
      "trainable parameter count:\n",
      "79073902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step        0]]     [[train]]     loss: 0.72492611       [[val]]     loss: 0.72382855       \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9982f5e45cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mnum_validation_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Training finished, start prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/Instacart_Market/models/rnn_product/../tf_base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 [val_loss] = self.session.run(\n\u001b[1;32m    141\u001b[0m                     \u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_feed_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 )\n\u001b[1;32m    144\u001b[0m                 \u001b[0mval_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_dir = './'\n",
    "\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, 'data'))\n",
    "\n",
    "nn = rnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs_bmm'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints_bmm'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions_bmm'),\n",
    "    optimizer='adam',\n",
    "    learning_rate=.001,\n",
    "    lstm_size=300,\n",
    "    batch_size=128,\n",
    "    num_training_steps=800,\n",
    "    early_stopping_steps=100,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    keep_prob=0.5,\n",
    "    enable_parameter_averaging=False,\n",
    "    num_restarts=2,\n",
    "    min_steps_to_checkpoint=200,\n",
    "    log_interval=20,\n",
    "    num_validation_batches=4,\n",
    ")\n",
    "nn.fit() # Training finished, start prediction\n",
    "nn.restore()\n",
    "nn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac97bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
