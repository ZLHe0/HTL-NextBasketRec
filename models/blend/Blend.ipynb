{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de32f17",
   "metadata": {},
   "source": [
    "## Collect Model Output Data and Generate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72159fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d267fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv('../../data/processed/product_data.csv')\n",
    "# Remove floats\n",
    "product_df = product_df.loc[product_df['product_name'].apply(lambda x: isinstance(x, str)),:]\n",
    "product_df = product_df.loc[product_df['is_ordered_history'].apply(lambda x: isinstance(x, str)),:]\n",
    "product_df = product_df[[\"user_id\", \"product_id\", \"label\"]] # usecols=['user_id', 'product_id', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e497be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7829, 1: 1223, -1: 262629}\n"
     ]
    }
   ],
   "source": [
    "# Check Data Shape\n",
    "product_df.shape\n",
    "# Check Label type\n",
    "label_col = product_df['label']\n",
    "label_set = set(label_col)\n",
    "label_count = {i: sum(label_col == i) for i in label_set}\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f24de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('../../data/raw/products.csv')\n",
    "product_df = product_df.merge(products, how='left', on='product_id')\n",
    "\n",
    "orders = pd.read_csv('../../data/raw/orders.csv')\n",
    "orders = orders[orders['eval_set'].isin({'train', 'test'})]\n",
    "product_df = product_df.merge(orders[['user_id', 'order_id']], how='left', on='user_id').reset_index(drop=True)\n",
    "product_df['is_none'] = (product_df['product_id'] == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e4162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn feature representations\n",
    "prefix = 'rnn_product'\n",
    "h_df = pd.DataFrame(np.load('../rnn_product/predictions/final_states.npy')).add_prefix('{}_h'.format(prefix))\n",
    "h_df['user_id'] = np.load('../rnn_product/predictions/user_ids.npy')\n",
    "h_df['product_id'] = np.load('../rnn_product/predictions/product_ids.npy')\n",
    "product_df = product_df.merge(h_df, how='left', on=['user_id', 'product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'label',\n",
    "    'user_id',\n",
    "    'product_id',\n",
    "    'order_id',\n",
    "    'product_name',\n",
    "    'aisle_id',\n",
    "    'department_id',\n",
    "]\n",
    "user_id = product_df['user_id']\n",
    "product_id = product_df['product_id']\n",
    "order_id = product_df['order_id']\n",
    "label = product_df['label']\n",
    "\n",
    "product_df.drop(drop_cols, axis=1, inplace=True)\n",
    "features = product_df.values\n",
    "feature_names = product_df.columns.values\n",
    "feature_maxs = features.max(axis=0)\n",
    "feature_mins = features.min(axis=0)\n",
    "feature_means = features.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8806d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features\n",
    "if not os.path.isdir('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "np.save('data/user_id.npy', user_id)\n",
    "np.save('data/product_id.npy', product_id)\n",
    "np.save('data/order_id.npy', order_id)\n",
    "np.save('data/features.npy', features)\n",
    "np.save('data/feature_names.npy', product_df.columns)\n",
    "np.save('data/feature_maxs.npy', feature_maxs)\n",
    "np.save('data/feature_mins.npy', feature_mins)\n",
    "np.save('data/feature_means.npy', feature_means)\n",
    "np.save('data/label.npy', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47da7dc",
   "metadata": {},
   "source": [
    "## NN Blend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28bc1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from data_frame import DataFrame\n",
    "from tf_base_model import TFBaseModel\n",
    "from tf_utils import dense_layer, log_loss\n",
    "\n",
    "\n",
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'order_id',\n",
    "            'product_id',\n",
    "            'features',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        df = DataFrame(columns=data_cols, data=data)\n",
    "        self.data_dim = df['features'].shape[1]\n",
    "\n",
    "        print(df.shapes())\n",
    "        print('loaded data')\n",
    "\n",
    "        # Since we don't have the true label for Kaggle test dataset\n",
    "        # we generate the test dataset by splitting the training dataset\n",
    "        ####################\n",
    "        df = df.mask(df['label'] != -1)\n",
    "        self.train_val_df, self.test_df = df.train_test_split(train_size=0.8)\n",
    "        self.train_df, self.val_df = self.train_val_df.train_test_split(train_size=0.9)\n",
    "        ####################\n",
    "\n",
    "        print( 'train size', len(self.train_df))\n",
    "        print( 'val size', len(self.val_df))\n",
    "        print( 'test size', len(self.test_df))\n",
    "\n",
    "        self.feature_means = np.load(os.path.join(data_dir, 'feature_means.npy'))\n",
    "        self.feature_maxs = np.load(os.path.join(data_dir, 'feature_maxs.npy'))\n",
    "        self.feature_mins = np.load(os.path.join(data_dir, 'feature_mins.npy'))\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            batch['features'] = np.nan_to_num((batch['features'] - self.feature_means) / (self.feature_maxs - self.feature_mins))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "646b3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn(TFBaseModel):\n",
    "\n",
    "    def __init__(self, hidden_units=500, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        super(nn, self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        self.order_id = tf.placeholder(tf.int32, [None])\n",
    "        self.product_id = tf.placeholder(tf.int32, [None])\n",
    "        self.features = tf.placeholder(tf.float32, [None, self.reader.data_dim])\n",
    "        self.label = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        h = dense_layer(self.features, self.hidden_units, activation=tf.nn.relu, scope='dense1')\n",
    "        h = tf.concat([h, self.features], axis=1)\n",
    "        y_hat = tf.squeeze(dense_layer(h, 1, activation=tf.nn.sigmoid, scope='dense2'), 1)\n",
    "        loss = log_loss(self.label, y_hat)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'order_ids': self.order_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'predictions': y_hat,\n",
    "            'labels': self.label\n",
    "        }\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9195bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "new run with parameters:\n",
      "{'batch_size': 128,\n",
      " 'checkpoint_dir': './checkpoints_nn',\n",
      " 'early_stopping_steps': 300,\n",
      " 'enable_parameter_averaging': False,\n",
      " 'grad_clip': 5,\n",
      " 'hidden_units': 64,\n",
      " 'keep_prob_scalar': 1.0,\n",
      " 'learning_rate': 0.005,\n",
      " 'log_dir': './logs_nn',\n",
      " 'log_interval': 20,\n",
      " 'loss_averaging_window': 100,\n",
      " 'min_steps_to_checkpoint': 100,\n",
      " 'num_restarts': 0,\n",
      " 'num_training_steps': 2000,\n",
      " 'num_validation_batches': 2,\n",
      " 'optimizer': 'adam',\n",
      " 'prediction_dir': './predictions_nn',\n",
      " 'reader': <__main__.DataReader object at 0x2aedb8ee3940>,\n",
      " 'regularization_constant': 0.0,\n",
      " 'warm_start_init_step': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id         (289024,)\n",
      "product_id       (289024,)\n",
      "features      (289024, 51)\n",
      "label            (289024,)\n",
      "dtype: object\n",
      "loaded data\n",
      "train size 7024\n",
      "val size 781\n",
      "test size 1952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "all parameters:\n",
      "[('dense1/weights:0', [51, 64]),\n",
      " ('dense1/biases:0', [64]),\n",
      " ('dense2/weights:0', [115, 1]),\n",
      " ('dense2/biases:0', [1]),\n",
      " ('Variable:0', []),\n",
      " ('Variable_1:0', []),\n",
      " ('beta1_power:0', []),\n",
      " ('beta2_power:0', []),\n",
      " ('dense1/weights/Adam:0', [51, 64]),\n",
      " ('dense1/weights/Adam_1:0', [51, 64]),\n",
      " ('dense1/biases/Adam:0', [64]),\n",
      " ('dense1/biases/Adam_1:0', [64]),\n",
      " ('dense2/weights/Adam:0', [115, 1]),\n",
      " ('dense2/weights/Adam_1:0', [115, 1]),\n",
      " ('dense2/biases/Adam:0', [1]),\n",
      " ('dense2/biases/Adam_1:0', [1])]\n",
      "trainable parameters:\n",
      "[('dense1/weights:0', [51, 64]),\n",
      " ('dense1/biases:0', [64]),\n",
      " ('dense2/weights:0', [115, 1]),\n",
      " ('dense2/biases:0', [1])]\n",
      "trainable parameter count:\n",
      "3444\n",
      "[[step        0]]     [[train]]     loss: 0.69173574       [[val]]     loss: 0.69281209       \n",
      "[[step       20]]     [[train]]     loss: 0.59147092       [[val]]     loss: 0.59358677       \n",
      "[[step       40]]     [[train]]     loss: 0.48803423       [[val]]     loss: 0.49711335       \n",
      "[[step       60]]     [[train]]     loss: 0.42270508       [[val]]     loss: 0.43090992       \n",
      "[[step       80]]     [[train]]     loss: 0.38129758       [[val]]     loss: 0.3922505        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[[step      100]]     [[train]]     loss: 0.3528045        [[val]]     loss: 0.36534492       \n",
      "[[step      120]]     [[train]]     loss: 0.28968921       [[val]]     loss: 0.30163035       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      140]]     [[train]]     loss: 0.26592089       [[val]]     loss: 0.27718664       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      160]]     [[train]]     loss: 0.25677417       [[val]]     loss: 0.27325503       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      180]]     [[train]]     loss: 0.25658396       [[val]]     loss: 0.27246476       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      200]]     [[train]]     loss: 0.25885595       [[val]]     loss: 0.27242188       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      220]]     [[train]]     loss: 0.25665928       [[val]]     loss: 0.27326019       \n",
      "[[step      240]]     [[train]]     loss: 0.25233751       [[val]]     loss: 0.27257384       \n",
      "[[step      260]]     [[train]]     loss: 0.25537374       [[val]]     loss: 0.27207462       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      280]]     [[train]]     loss: 0.25790385       [[val]]     loss: 0.2721367        \n",
      "[[step      300]]     [[train]]     loss: 0.25701073       [[val]]     loss: 0.27202591       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      320]]     [[train]]     loss: 0.25941772       [[val]]     loss: 0.27168702       \n",
      "saving model to ./checkpoints_nn/model\n",
      "[[step      340]]     [[train]]     loss: 0.26478484       [[val]]     loss: 0.27256449       \n",
      "[[step      360]]     [[train]]     loss: 0.26314407       [[val]]     loss: 0.27199326       \n",
      "[[step      380]]     [[train]]     loss: 0.2608948        [[val]]     loss: 0.27283483       \n",
      "[[step      400]]     [[train]]     loss: 0.26215582       [[val]]     loss: 0.27296921       \n",
      "[[step      420]]     [[train]]     loss: 0.25933101       [[val]]     loss: 0.27269856       \n",
      "[[step      440]]     [[train]]     loss: 0.25750843       [[val]]     loss: 0.27296214       \n",
      "[[step      460]]     [[train]]     loss: 0.25898533       [[val]]     loss: 0.27333673       \n",
      "[[step      480]]     [[train]]     loss: 0.25874903       [[val]]     loss: 0.2726837        \n",
      "[[step      500]]     [[train]]     loss: 0.25588176       [[val]]     loss: 0.27324813       \n",
      "[[step      520]]     [[train]]     loss: 0.25732183       [[val]]     loss: 0.27332445       \n",
      "[[step      540]]     [[train]]     loss: 0.25745036       [[val]]     loss: 0.27342982       \n",
      "[[step      560]]     [[train]]     loss: 0.25665847       [[val]]     loss: 0.27325829       \n",
      "[[step      580]]     [[train]]     loss: 0.2563774        [[val]]     loss: 0.27372741       \n",
      "[[step      600]]     [[train]]     loss: 0.25953473       [[val]]     loss: 0.27263117       \n",
      "[[step      620]]     [[train]]     loss: 0.25955903       [[val]]     loss: 0.27300247       \n",
      "[[step      640]]     [[train]]     loss: 0.25887136       [[val]]     loss: 0.27218708       \n",
      "best validation loss of 0.27168701648712157 at training step 320\n",
      "early stopping - ending training.\n",
      "restoring model parameters from ./checkpoints_nn/model-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints_nn/model-320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "saving order_ids with shape (1952,) to ./predictions_nn/order_ids.npy\n",
      "saving product_ids with shape (1952,) to ./predictions_nn/product_ids.npy\n",
      "saving predictions with shape (1952,) to ./predictions_nn/predictions.npy\n",
      "saving labels with shape (1952,) to ./predictions_nn/labels.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "base_dir = './'\n",
    "\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, 'data'))\n",
    "\n",
    "# A tiny one for this small dataset\n",
    "nn = nn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs_nn'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints_nn'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions_nn'),\n",
    "    optimizer='adam',\n",
    "    learning_rate=.005,\n",
    "    hidden_units=64,\n",
    "    batch_size=128,\n",
    "    num_training_steps=2000,\n",
    "    early_stopping_steps=300,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    keep_prob=1.0,\n",
    "    enable_parameter_averaging=False,\n",
    "    num_restarts=0,\n",
    "    min_steps_to_checkpoint=100,\n",
    "    log_interval=20,\n",
    "    num_validation_batches=2,\n",
    ")\n",
    "nn.fit()\n",
    "nn.restore()\n",
    "nn.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7c553",
   "metadata": {},
   "source": [
    "## GBM Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9f4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pprint as pp\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44a45a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "order_id = np.load('data/order_id.npy')\n",
    "product_id = np.load('data/product_id.npy')\n",
    "features = np.load('data/features.npy')\n",
    "feature_names = np.load('data/feature_names.npy', allow_pickle=True)\n",
    "label = np.load('data/label.npy')\n",
    "\n",
    "product_df = pd.DataFrame(data=features, columns=feature_names)\n",
    "product_df['order_id'] = order_id\n",
    "product_df['product_id'] = product_id\n",
    "product_df['label'] = label\n",
    "\n",
    "del order_id, product_id, features, feature_names, label\n",
    "gc.collect()\n",
    "\n",
    "drop_cols = [i for i in product_df.columns if i.startswith('sgns') or i.startswith('nnmf')]\n",
    "drop_cols += ['order_id', 'product_id', 'label']\n",
    "\n",
    "# training\n",
    "\n",
    "#self.train_val_df, self.test_df = df.train_test_split(train_size=0.8)\n",
    "#self.train_df, self.val_df = self.train_val_df.train_test_split(train_size=0.9)\n",
    "df = product_df[product_df['label'] != -1]\n",
    "train_val_df, test_df = train_test_split(df, train_size=0.8)\n",
    "train_df, val_df = train_test_split(train_val_df, train_size=0.9)\n",
    "del product_df\n",
    "gc.collect()\n",
    "\n",
    "Y_train, Y_val = train_df['label'].astype(int).astype(float), val_df['label'].astype(int).astype(float)\n",
    "X_train, X_val = train_df.drop(drop_cols, axis=1), val_df.drop(drop_cols, axis=1)\n",
    "del train_df\n",
    "gc.collect()\n",
    "\n",
    "test_orders = test_df['order_id']\n",
    "test_products = test_df['product_id']\n",
    "test_labels = test_df['label']\n",
    "X_test = test_df.drop(drop_cols, axis=1)\n",
    "del test_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77606816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/storage/work/z/zbh5185/myenv/lib64/python3.6/site-packages/lightgbm/basic.py:1491: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via 'params' instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 877, number of negative: 6147\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8451\n",
      "[LightGBM] [Info] Number of data points in the train set: 7024, number of used features: 51\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[20]\ttrain's binary_logloss: 0.336001\tvalid's binary_logloss: 0.346389\n",
      "[40]\ttrain's binary_logloss: 0.310622\tvalid's binary_logloss: 0.324106\n",
      "[60]\ttrain's binary_logloss: 0.291408\tvalid's binary_logloss: 0.307751\n",
      "[80]\ttrain's binary_logloss: 0.276505\tvalid's binary_logloss: 0.295736\n",
      "[100]\ttrain's binary_logloss: 0.26455\tvalid's binary_logloss: 0.286133\n",
      "[120]\ttrain's binary_logloss: 0.255257\tvalid's binary_logloss: 0.27897\n",
      "[140]\ttrain's binary_logloss: 0.247238\tvalid's binary_logloss: 0.273076\n",
      "[160]\ttrain's binary_logloss: 0.240503\tvalid's binary_logloss: 0.268895\n",
      "[180]\ttrain's binary_logloss: 0.234675\tvalid's binary_logloss: 0.265638\n",
      "[200]\ttrain's binary_logloss: 0.229565\tvalid's binary_logloss: 0.263207\n",
      "[220]\ttrain's binary_logloss: 0.224672\tvalid's binary_logloss: 0.260929\n",
      "[240]\ttrain's binary_logloss: 0.220323\tvalid's binary_logloss: 0.25898\n",
      "[260]\ttrain's binary_logloss: 0.216634\tvalid's binary_logloss: 0.25778\n",
      "[280]\ttrain's binary_logloss: 0.213185\tvalid's binary_logloss: 0.257114\n",
      "[300]\ttrain's binary_logloss: 0.209481\tvalid's binary_logloss: 0.256408\n",
      "[320]\ttrain's binary_logloss: 0.206244\tvalid's binary_logloss: 0.256073\n",
      "[340]\ttrain's binary_logloss: 0.203248\tvalid's binary_logloss: 0.255531\n",
      "[360]\ttrain's binary_logloss: 0.200039\tvalid's binary_logloss: 0.255611\n",
      "[380]\ttrain's binary_logloss: 0.196979\tvalid's binary_logloss: 0.254885\n",
      "[400]\ttrain's binary_logloss: 0.194169\tvalid's binary_logloss: 0.254921\n",
      "[420]\ttrain's binary_logloss: 0.191567\tvalid's binary_logloss: 0.254775\n",
      "[440]\ttrain's binary_logloss: 0.188834\tvalid's binary_logloss: 0.255006\n",
      "[460]\ttrain's binary_logloss: 0.18643\tvalid's binary_logloss: 0.254778\n",
      "[480]\ttrain's binary_logloss: 0.183939\tvalid's binary_logloss: 0.25496\n",
      "[500]\ttrain's binary_logloss: 0.181572\tvalid's binary_logloss: 0.25521\n",
      "[520]\ttrain's binary_logloss: 0.179418\tvalid's binary_logloss: 0.254688\n",
      "[540]\ttrain's binary_logloss: 0.177324\tvalid's binary_logloss: 0.254707\n",
      "[560]\ttrain's binary_logloss: 0.175344\tvalid's binary_logloss: 0.254753\n",
      "[580]\ttrain's binary_logloss: 0.173347\tvalid's binary_logloss: 0.25451\n",
      "[600]\ttrain's binary_logloss: 0.171299\tvalid's binary_logloss: 0.254365\n",
      "[620]\ttrain's binary_logloss: 0.16951\tvalid's binary_logloss: 0.254876\n",
      "[640]\ttrain's binary_logloss: 0.16738\tvalid's binary_logloss: 0.255322\n",
      "[660]\ttrain's binary_logloss: 0.165443\tvalid's binary_logloss: 0.255129\n",
      "[680]\ttrain's binary_logloss: 0.163538\tvalid's binary_logloss: 0.255316\n",
      "[700]\ttrain's binary_logloss: 0.161982\tvalid's binary_logloss: 0.255679\n",
      "[720]\ttrain's binary_logloss: 0.160237\tvalid's binary_logloss: 0.256008\n",
      "[740]\ttrain's binary_logloss: 0.158598\tvalid's binary_logloss: 0.255958\n",
      "[760]\ttrain's binary_logloss: 0.157063\tvalid's binary_logloss: 0.256077\n",
      "[780]\ttrain's binary_logloss: 0.155284\tvalid's binary_logloss: 0.256127\n",
      "[800]\ttrain's binary_logloss: 0.153581\tvalid's binary_logloss: 0.256719\n",
      "[820]\ttrain's binary_logloss: 0.151869\tvalid's binary_logloss: 0.256796\n",
      "[840]\ttrain's binary_logloss: 0.150417\tvalid's binary_logloss: 0.256651\n",
      "[860]\ttrain's binary_logloss: 0.148618\tvalid's binary_logloss: 0.256912\n",
      "[880]\ttrain's binary_logloss: 0.147203\tvalid's binary_logloss: 0.257256\n",
      "[900]\ttrain's binary_logloss: 0.145815\tvalid's binary_logloss: 0.257254\n",
      "[920]\ttrain's binary_logloss: 0.144429\tvalid's binary_logloss: 0.257586\n",
      "[940]\ttrain's binary_logloss: 0.142962\tvalid's binary_logloss: 0.25788\n",
      "[960]\ttrain's binary_logloss: 0.141483\tvalid's binary_logloss: 0.258164\n",
      "[980]\ttrain's binary_logloss: 0.140325\tvalid's binary_logloss: 0.258552\n",
      "[1000]\ttrain's binary_logloss: 0.138951\tvalid's binary_logloss: 0.258822\n",
      "[1020]\ttrain's binary_logloss: 0.137721\tvalid's binary_logloss: 0.258752\n",
      "[1040]\ttrain's binary_logloss: 0.136315\tvalid's binary_logloss: 0.259129\n",
      "[1060]\ttrain's binary_logloss: 0.135009\tvalid's binary_logloss: 0.259314\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttrain's binary_logloss: 0.173725\tvalid's binary_logloss: 0.254266\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'learning_rate': .01,\n",
    "    'num_leaves': 32,\n",
    "    'max_depth': 6,\n",
    "    'feature_fraction': 0.35,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 2,\n",
    "    'early_stopping_round': 500\n",
    "}\n",
    "rounds = 15000\n",
    "d_train = lgb.Dataset(X_train, label=Y_train, silent=True)\n",
    "d_valid = lgb.Dataset(X_val, label=Y_val, silent=True)\n",
    "del X_train, X_val, Y_train, Y_val\n",
    "\n",
    "valid_sets = [d_train, d_valid]\n",
    "valid_names = ['train', 'valid']\n",
    "gbdt = lgb.train(params, d_train, rounds, valid_sets=valid_sets, valid_names=valid_names, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a5c77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rnn_product_h7', 0.0),\n",
      " ('rnn_product_h9', 0.0),\n",
      " ('rnn_product_h10', 0.0),\n",
      " ('rnn_product_h12', 0.0),\n",
      " ('rnn_product_h17', 0.0),\n",
      " ('rnn_product_h18', 0.0),\n",
      " ('rnn_product_h21', 0.0),\n",
      " ('rnn_product_h28', 0.0),\n",
      " ('rnn_product_h30', 0.0),\n",
      " ('rnn_product_h33', 0.0),\n",
      " ('rnn_product_h36', 0.0),\n",
      " ('rnn_product_h39', 0.0),\n",
      " ('rnn_product_h47', 0.0),\n",
      " ('rnn_product_h48', 0.0),\n",
      " ('rnn_product_h49', 0.0),\n",
      " ('rnn_product_h5', 0.00018249285236328244),\n",
      " ('rnn_product_h0', 0.0012774499665429771),\n",
      " ('is_none', 0.012348683009915444),\n",
      " ('rnn_product_h44', 0.02238578988989598),\n",
      " ('rnn_product_h2', 0.023054930348561348),\n",
      " ('rnn_product_h6', 0.02311576129934911),\n",
      " ('rnn_product_h34', 0.02311576129934911),\n",
      " ('rnn_product_h15', 0.024940689822981935),\n",
      " ('rnn_product_h46', 0.024940689822981935),\n",
      " ('rnn_product_h29', 0.02694811119897804),\n",
      " ('rnn_product_h31', 0.027252265952916844),\n",
      " ('rnn_product_h1', 0.02755642070685565),\n",
      " ('rnn_product_h35', 0.027982237362369974),\n",
      " ('rnn_product_h14', 0.028043068313157733),\n",
      " ('rnn_product_h20', 0.028043068313157733),\n",
      " ('rnn_product_h38', 0.0284080540178843),\n",
      " ('rnn_product_h11', 0.02859054687024758),\n",
      " ('rnn_product_h24', 0.02919885637812519),\n",
      " ('rnn_product_h16', 0.029259687328912952),\n",
      " ('rnn_product_h22', 0.029503011132063994),\n",
      " ('rnn_product_h27', 0.02992882778757832),\n",
      " ('rnn_product_h23', 0.03005048968915384),\n",
      " ('rnn_product_h32', 0.031084615852545776),\n",
      " ('rnn_product_h43', 0.031327939655696815),\n",
      " ('rnn_product_h45', 0.03211874201593771),\n",
      " ('rnn_product_h8', 0.03266622057302756),\n",
      " ('rnn_product_h40', 0.03266622057302756),\n",
      " ('rnn_product_h41', 0.033457022933268445),\n",
      " ('rnn_product_h42', 0.034004501490358295),\n",
      " ('rnn_product_h13', 0.034065332441146054),\n",
      " ('rnn_product_h4', 0.03485613480138695),\n",
      " ('rnn_product_h26', 0.034916965752174706),\n",
      " ('rnn_product_h19', 0.034977796702962465),\n",
      " ('rnn_product_h3', 0.035525275260052315),\n",
      " ('rnn_product_h25', 0.035586106210840074),\n",
      " ('rnn_product_h37', 0.03662023237423201)]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and Predict\n",
    "features = gbdt.feature_name()\n",
    "importance = list(gbdt.feature_importance())\n",
    "importance = zip(features, importance)\n",
    "importance = sorted(importance, key=lambda x: x[1])\n",
    "total = sum(j for i, j in importance)\n",
    "importance = [(i, float(j)/total) for i, j in importance]\n",
    "pp.pprint(importance)\n",
    "\n",
    "test_preds = gbdt.predict(X_test, num_iteration=gbdt.best_iteration)\n",
    "\n",
    "dirname = 'predictions_gbm'\n",
    "if not os.path.isdir(dirname):\n",
    "    os.makedirs(dirname)\n",
    "\n",
    "np.save(os.path.join(dirname, 'order_ids.npy'), test_orders)\n",
    "np.save(os.path.join(dirname, 'product_ids.npy'), test_products)\n",
    "np.save(os.path.join(dirname, 'predictions.npy'), test_preds)\n",
    "np.save(os.path.join(dirname, 'labels.npy'), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c14ae",
   "metadata": {},
   "source": [
    "## F-score Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d2195c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1d375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "gbm_df = pd.DataFrame({\n",
    "    'order_id': np.load('predictions_gbm/order_ids.npy'),\n",
    "    'product_id': np.load('predictions_gbm/product_ids.npy'),\n",
    "    'prediction_gbm': np.load('predictions_gbm/predictions.npy'),\n",
    "    'label': np.load('predictions_gbm/labels.npy')\n",
    "})\n",
    "\n",
    "nn_df = pd.DataFrame({\n",
    "    'order_id': np.load('predictions_nn/order_ids.npy'),\n",
    "    'product_id': np.load('predictions_nn/product_ids.npy'),\n",
    "    'prediction_nn': np.load('predictions_nn/predictions.npy'),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4e944f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the prediction\n",
    "# pred_df = gbm_df.merge(nn_df, how='left', on=['order_id', 'product_id'])\n",
    "# pred_df['prediction'] = .9*pred_df['prediction_gbm'] + .1*pred_df['prediction_nn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48fab795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model    recall  precision   F_value\n",
      "0   GBM  0.175781   0.555556  0.267062\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0b5da273364d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpred_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction_nn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_f_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-0b5da273364d>\u001b[0m in \u001b[0;36mcal_f_value\u001b[0;34m(pred, label, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mF_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrecall\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Calculate F-score\n",
    "def cal_f_value(pred, label, model):\n",
    "    TP = sum((label==1) & (pred==1))\n",
    "    FP = sum((pred==1) & (label==0))\n",
    "    FN = sum((pred==0) & (label==1))\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    F_value = 2 / (1/recall + 1/precision)\n",
    "    summary = [model, recall, precision, F_value]\n",
    "    summary_name = [\"model\", \"recall\", \"precision\", \"F_value\"]\n",
    "    return(pd.DataFrame(dict(zip(summary_name, summary)), index=[0]))\n",
    "\n",
    "true_label = gbm_df['label']\n",
    "# gbm\n",
    "pred_gbm = (gbm_df['prediction_gbm'] > 0.5).astype(int)\n",
    "print(cal_f_value(pred_gbm, true_label, \"GBM\"))\n",
    "# nn\n",
    "pred_nn = (nn_df['prediction_nn'] > 0.5).astype(int)\n",
    "print(cal_f_value(pred_nn, true_label, \"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a44f1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "1947    0\n",
       "1948    0\n",
       "1949    0\n",
       "1950    0\n",
       "1951    0\n",
       "Name: prediction_nn, Length: 1952, dtype: int64>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nn.unique # All Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf6349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
