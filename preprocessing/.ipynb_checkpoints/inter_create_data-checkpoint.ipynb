{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39106823",
   "metadata": {},
   "source": [
    "## User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output # Count iteration times\n",
    "# from datetime import datetime # Record time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866a9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_order(x):\n",
    "    series = pd.Series(dtype='object') ### specify dtype\n",
    "\n",
    "    series['products'] = '_'.join(x['product_id'].values.astype(str).tolist())\n",
    "    series['reorders'] = '_'.join(x['reordered'].values.astype(str).tolist())\n",
    "    series['aisles'] = '_'.join(x['aisle_id'].values.astype(str).tolist())\n",
    "    series['departments'] = '_'.join(x['department_id'].values.astype(str).tolist())\n",
    "\n",
    "    series['order_number'] = x['order_number'].iloc[0]\n",
    "    series['order_dow'] = x['order_dow'].iloc[0]\n",
    "    series['order_hour'] = x['order_hour_of_day'].iloc[0]\n",
    "    series['days_since_prior_order'] = x['days_since_prior_order'].iloc[0]\n",
    "    \n",
    "    # Increment the counter and print the current count\n",
    "    global global_counter \n",
    "    global_counter += 1\n",
    "    clear_output(wait=True)\n",
    "    print(f\"total loops run: {global_counter}\")\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5cf04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user(x):\n",
    "    parsed_orders = x.groupby('order_id', sort=False).apply(parse_order)\n",
    "\n",
    "    series = pd.Series(dtype='object')\n",
    "\n",
    "    series['order_ids'] = ' '.join(parsed_orders.index.map(str).tolist())\n",
    "    series['order_numbers'] = ' '.join(parsed_orders['order_number'].map(str).tolist())\n",
    "    series['order_dows'] = ' '.join(parsed_orders['order_dow'].map(str).tolist())\n",
    "    series['order_hours'] = ' '.join(parsed_orders['order_hour'].map(str).tolist())\n",
    "    series['days_since_prior_orders'] = ' '.join(parsed_orders['days_since_prior_order'].map(str).tolist())\n",
    "\n",
    "    series['product_ids'] = ' '.join(parsed_orders['products'].values.astype(str).tolist())\n",
    "    series['aisle_ids'] = ' '.join(parsed_orders['aisles'].values.astype(str).tolist())\n",
    "    series['department_ids'] = ' '.join(parsed_orders['departments'].values.astype(str).tolist())\n",
    "    series['reorders'] = ' '.join(parsed_orders['reorders'].values.astype(str).tolist())\n",
    "\n",
    "    series['eval_set'] = x['eval_set'].values[-1]\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a07dd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/work/z/zbh5185/Instacart_Market/preprocessing'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() # Check the current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f5c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('../data/raw/orders.csv')\n",
    "prior_products = pd.read_csv('../data/raw/order_products__prior.csv')\n",
    "train_products = pd.read_csv('../data/raw/order_products__train.csv')\n",
    "order_products = pd.concat([prior_products, train_products], axis=0)\n",
    "products = pd.read_csv('../data/raw/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333c438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orders.merge(order_products, how='left', on='order_id')\n",
    "df = df.merge(products, how='left', on='product_id')\n",
    "df['days_since_prior_order'] = df['days_since_prior_order'].fillna(0).astype(int)\n",
    "null_cols = ['product_id', 'aisle_id', 'department_id', 'add_to_cart_order', 'reordered']\n",
    "df[null_cols] = df[null_cols].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e0a004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32434489, 4)\n",
      "(1384617, 4)\n",
      "['prior' 'train' 'test']\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Check\n",
    "print(prior_products.shape)\n",
    "print(train_products.shape)\n",
    "print(df['eval_set'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b610639",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('../data/processed'):\n",
    "    os.makedirs('../data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8be49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Goal: sample from df, with each aisle and department are sampled at least tenth.\n",
    "\n",
    "# Identify all unique aisles and departments\n",
    "unique_aisles = df['aisle_id'].unique()\n",
    "unique_departments = df['department_id'].unique()\n",
    "\n",
    "# Initialize an empty dataframe to store the sampled data\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Sample at least ten rows for each aisle and department\n",
    "for aisle in unique_aisles:\n",
    "    sampled_df = sampled_df.append(df[df['aisle_id'] == aisle].sample(n=10))\n",
    "\n",
    "for department in unique_departments:\n",
    "    sampled_df = sampled_df.append(df[df['department_id'] == department].sample(n=10))\n",
    "\n",
    "# Calculate the remaining number of rows needed to make up 1% of the data\n",
    "remaining_rows = int(0.01 * len(df)) - len(sampled_df)\n",
    "\n",
    "# Sample the remaining rows randomly from the data\n",
    "remaining_df = df.drop(sampled_df.index)\n",
    "remaining_df_train = remaining_df[remaining_df['eval_set'] == 'train'] # Mainly sample training data\n",
    "remaining_sampled_df_train = remaining_df_train.sample(n=int(0.99*remaining_rows))\n",
    "\n",
    "remaining_df_test = remaining_df[remaining_df['eval_set'] != 'train'] # Include some other data\n",
    "remaining_sampled_df_test = remaining_df_test.sample(n=int(0.99*remaining_rows))\n",
    "\n",
    "# Append the remaining sampled rows to the sampled dataframe\n",
    "sampled_df = pd.concat([sampled_df, remaining_sampled_df_train, remaining_sampled_df_test])\n",
    "\n",
    "# Reset the index of the sampled dataframe\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ecac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops run: 76769\n"
     ]
    }
   ],
   "source": [
    "# user_data = df.groupby('user_id', sort=False).apply(parse_user).reset_index()\n",
    "# Initialize a global counter\n",
    "global_counter = 0\n",
    "user_data = sampled_df.groupby('user_id', sort=False).apply(parse_user).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e61eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.to_csv('../data/processed/user_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2f010",
   "metadata": {},
   "source": [
    "##  Product_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6084bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/user_data.csv')\n",
    "\n",
    "products = pd.read_csv('../data/raw/products.csv')\n",
    "product_to_aisle = dict(zip(products['product_id'], products['aisle_id']))\n",
    "product_to_department = dict(zip(products['product_id'], products['department_id']))\n",
    "product_to_name = dict(zip(products['product_id'], products['product_name']))\n",
    "\n",
    "user_ids = []\n",
    "product_ids = []\n",
    "aisle_ids = []\n",
    "department_ids = []\n",
    "product_names = []\n",
    "eval_sets = []\n",
    "\n",
    "is_ordered_histories = []\n",
    "index_in_order_histories = []\n",
    "order_size_histories = []\n",
    "reorder_size_histories = []\n",
    "order_dow_histories = []\n",
    "order_hour_histories = []\n",
    "days_since_prior_order_histories = []\n",
    "order_number_histories = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "longest = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b69defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "10000\n",
      "[56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753, 56753]\n",
      "20000\n",
      "[110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400, 110400]\n",
      "30000\n",
      "[160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340, 160340]\n",
      "40000\n",
      "[208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372, 208372]\n",
      "50000\n",
      "[252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955, 252955]\n",
      "60000\n",
      "[294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419, 294419]\n",
      "70000\n",
      "[332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499, 332499]\n",
      "80000\n",
      "[366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897, 366897]\n",
      "90000\n",
      "[397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897, 397897]\n",
      "100000\n",
      "[425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147, 425147]\n",
      "110000\n",
      "[452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618, 452618]\n",
      "120000\n",
      "[493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537, 493537]\n",
      "130000\n",
      "[524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926, 524926]\n",
      "140000\n",
      "[548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805, 548805]\n",
      "150000\n",
      "[566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658, 566658]\n",
      "160000\n",
      "[579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496, 579496]\n"
     ]
    }
   ],
   "source": [
    "longest = 0\n",
    "for _, row in df.iterrows():\n",
    "    if _ % 10000 == 0:\n",
    "        print(_)\n",
    "        data = [\n",
    "        user_ids,\n",
    "        product_ids,\n",
    "        aisle_ids,\n",
    "        department_ids,\n",
    "        product_names,\n",
    "        is_ordered_histories,\n",
    "        index_in_order_histories,\n",
    "        order_size_histories,\n",
    "        reorder_size_histories,\n",
    "        order_dow_histories,\n",
    "        order_hour_histories,\n",
    "        days_since_prior_order_histories,\n",
    "        order_number_histories,\n",
    "        labels,\n",
    "        eval_sets\n",
    "        ]\n",
    "        # Length Check\n",
    "        print(list(map(len, data)))\n",
    "\n",
    "    user_id = row['user_id']\n",
    "    eval_set = row['eval_set']\n",
    "    products = row['product_ids']\n",
    "\n",
    "    products, next_products = ' '.join(products.split()[:-1]), products.split()[-1]\n",
    "\n",
    "    reorders = row['reorders']\n",
    "    reorders, next_reorders = ' '.join(reorders.split()[:-1]), reorders.split()[-1]\n",
    "\n",
    "    product_set = set([int(j) for i in products.split() for j in i.split('_')])\n",
    "    next_product_set = set([int(i) for i in next_products.split('_')])\n",
    "\n",
    "    orders = [map(int, i.split('_')) for i in products.split()]\n",
    "    reorders = [map(int, i.split('_')) for i in reorders.split()]\n",
    "    next_reorders = map(int, next_reorders.split('_'))\n",
    "\n",
    "    for product_id in product_set:\n",
    "\n",
    "        user_ids.append(user_id)\n",
    "        product_ids.append(product_id)\n",
    "        labels.append(int(product_id in next_product_set) if eval_set == 'train' else -1)\n",
    "        eval_sets.append(eval_set) # Newly added\n",
    "\n",
    "        ### Handle Null\n",
    "        if product_id in product_to_aisle:\n",
    "            aisle_ids.append(product_to_aisle[product_id])\n",
    "        else:\n",
    "            aisle_ids.append('0')  # or some other default value\n",
    "\n",
    "        if product_id in product_to_department:\n",
    "            department_ids.append(product_to_department[product_id])\n",
    "        else:\n",
    "            department_ids.append('0')  # or some other default value\n",
    "\n",
    "        if product_id in product_to_name:\n",
    "            product_names.append(product_to_name[product_id])\n",
    "        else:\n",
    "            product_names.append('0')  # or some other default value\n",
    "\n",
    "        is_ordered = []\n",
    "        index_in_order = []\n",
    "        order_size = []\n",
    "        reorder_size = []\n",
    "\n",
    "        prior_products = set()\n",
    "        for order in orders:\n",
    "            is_ordered.append(str(int(product_id in order)))\n",
    "            index_in_order.append(str(order.index(product_id) + 1) if product_id in order else '0')\n",
    "            order_size.append(str(len(list(order))))\n",
    "            reorder_size.append(str(len(list(prior_products & set(order)))))\n",
    "            prior_products |= set(order)\n",
    "\n",
    "        is_ordered = ' '.join(is_ordered)\n",
    "        index_in_order = ' '.join(index_in_order)\n",
    "        order_size = ' '.join(order_size)\n",
    "        reorder_size = ' '.join(reorder_size)\n",
    "\n",
    "        is_ordered_histories.append(is_ordered)\n",
    "        index_in_order_histories.append(index_in_order)\n",
    "        order_size_histories.append(order_size)\n",
    "        reorder_size_histories.append(reorder_size)\n",
    "        order_dow_histories.append(row['order_dows'])\n",
    "        order_hour_histories.append(row['order_hours'])\n",
    "        days_since_prior_order_histories.append(row['days_since_prior_orders'])\n",
    "        order_number_histories.append(row['order_numbers'])\n",
    "\n",
    "    user_ids.append(user_id)\n",
    "    product_ids.append(0)\n",
    "    labels.append(int(max(next_reorders) == 0) if eval_set == 'train' else -1)\n",
    "\n",
    "    aisle_ids.append(0)\n",
    "    department_ids.append(0)\n",
    "    product_names.append(0)\n",
    "    eval_sets.append(eval_set)\n",
    "\n",
    "    is_ordered = []\n",
    "    index_in_order = []\n",
    "    order_size = []\n",
    "    reorder_size = []\n",
    "\n",
    "    for reorder in reorders:\n",
    "        is_ordered.append(str(int(max(reorder) == 0)))\n",
    "        index_in_order.append(str(0))\n",
    "        order_size.append(str(len(list(reorder))))\n",
    "        reorder_size.append(str(sum(reorder)))\n",
    "\n",
    "    is_ordered = ' '.join(is_ordered)\n",
    "    index_in_order = ' '.join(index_in_order)\n",
    "    order_size = ' '.join(order_size)\n",
    "    reorder_size = ' '.join(reorder_size)\n",
    "\n",
    "    is_ordered_histories.append(is_ordered)\n",
    "    index_in_order_histories.append(index_in_order)\n",
    "    order_size_histories.append(order_size)\n",
    "    reorder_size_histories.append(reorder_size)\n",
    "    order_dow_histories.append(row['order_dows'])\n",
    "    order_hour_histories.append(row['order_hours'])\n",
    "    days_since_prior_order_histories.append(row['days_since_prior_orders'])\n",
    "    order_number_histories.append(row['order_numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "541f62f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809,\n",
       " 581809]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "        user_ids,\n",
    "        product_ids,\n",
    "        aisle_ids,\n",
    "        department_ids,\n",
    "        product_names,\n",
    "        is_ordered_histories,\n",
    "        index_in_order_histories,\n",
    "        order_size_histories,\n",
    "        reorder_size_histories,\n",
    "        order_dow_histories,\n",
    "        order_hour_histories,\n",
    "        days_since_prior_order_histories,\n",
    "        order_number_histories,\n",
    "        labels,\n",
    "        eval_sets\n",
    "    ]\n",
    "# Length Check\n",
    "\n",
    "list(map(len, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7694d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'user_id',\n",
    "    'product_id',\n",
    "    'aisle_id',\n",
    "    'department_id',\n",
    "    'product_name',\n",
    "    'is_ordered_history',\n",
    "    'index_in_order_history',\n",
    "    'order_size_history',\n",
    "    'reorder_size_history',\n",
    "    'order_dow_history',\n",
    "    'order_hour_history',\n",
    "    'days_since_prior_order_history',\n",
    "    'order_number_history',\n",
    "    'label',\n",
    "    'eval_set'\n",
    "]\n",
    "if not os.path.isdir('../data/processed'):\n",
    "    os.makedirs('../data/processed')\n",
    "\n",
    "df = pd.DataFrame(dict(zip(columns, data)))\n",
    "df.to_csv('../data/processed/product_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98922e7e",
   "metadata": {},
   "source": [
    "### Added for Tonight Running, Delete After It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c650f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pad_1d(array, max_len):\n",
    "    array = list(array)[:max_len]\n",
    "    length = len(array)\n",
    "    padded = array + [0]*(max_len - len(array))\n",
    "    return padded, length\n",
    "\n",
    "\n",
    "def make_word_idx(product_names):\n",
    "    words = [word for name in product_names for word in name.split()]\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    max_id = 1\n",
    "    word_idx = {}\n",
    "    for word, count in word_counts.items():\n",
    "        if count < 10:\n",
    "            word_idx[word] = 0\n",
    "        else:\n",
    "            word_idx[word] = max_id\n",
    "            max_id += 1\n",
    "\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def encode_text(text, word_idx):\n",
    "    return ' '.join([str(word_idx[i]) for i in text.split()]) if text else '0'\n",
    "\n",
    "# Embedding debugging\n",
    "product_data = pd.read_csv('../data/processed/product_data.csv')\n",
    "# Remove floats\n",
    "product_data = product_data.loc[product_data['product_name'].apply(lambda x: isinstance(x, str)),:]\n",
    "product_data = product_data.loc[product_data['is_ordered_history'].apply(lambda x: isinstance(x, str)),:]\n",
    "\n",
    "product_data['product_name'] = product_data['product_name'].map(lambda x: x.lower() if type(x)==str else 0)\n",
    "\n",
    "product_df = pd.read_csv('../data/raw/products.csv')\n",
    "product_df['product_name'] = product_df['product_name'].map(lambda x: x.lower())\n",
    "\n",
    "word_idx = make_word_idx(product_df['product_name'].tolist())\n",
    "product_data['product_name_encoded'] = product_data['product_name'].map(lambda x: encode_text(x, word_idx))\n",
    "\n",
    "# Check Data Shape\n",
    "product_data.shape\n",
    "# Check Label type\n",
    "label_col = product_data['label']\n",
    "label_set = set(label_col)\n",
    "label_count = {i: sum(label_col == i) for i in label_set}\n",
    "print(label_count)\n",
    "\n",
    "# Check the property of list product_name\n",
    "mixlist = product_data['product_name']\n",
    "mixlist_type = [type(s) for s in mixlist]\n",
    "mixlist_type_dic = {t: mixlist_type.count(t) for t in set(mixlist_type)}\n",
    "print(mixlist_type_dic)\n",
    "\n",
    "num_rows = len(product_data)\n",
    "\n",
    "user_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "product_id = np.zeros(shape=[num_rows], dtype=np.int32)\n",
    "aisle_id = np.zeros(shape=[num_rows], dtype=np.int16)\n",
    "department_id = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "eval_set = np.zeros(shape=[num_rows], dtype='S5')\n",
    "label = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "\n",
    "is_ordered_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "index_in_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_dow_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_hour_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "days_since_prior_order_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "reorder_size_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "order_number_history = np.zeros(shape=[num_rows, 100], dtype=np.int8)\n",
    "product_name = np.zeros(shape=[num_rows, 30], dtype=np.int32)\n",
    "product_name_length = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "history_length = np.zeros(shape=[num_rows], dtype=np.int8)\n",
    "\n",
    "# Length check\n",
    "print(user_id.shape, is_ordered_history.shape, order_dow_history.shape)\n",
    "\n",
    "np.save('../models/rnn_product/data/user_id.npy', user_id)\n",
    "np.save('../models/rnn_product/data/product_id.npy', product_id)\n",
    "np.save('../models/rnn_product/data/aisle_id.npy', aisle_id)\n",
    "np.save('../models/rnn_product/data/department_id.npy', department_id)\n",
    "np.save('../models/rnn_product/data/eval_set.npy', eval_set)\n",
    "np.save('../models/rnn_product/data/label.npy', label)\n",
    "\n",
    "np.save('../models/rnn_product/data/is_ordered_history.npy', is_ordered_history)\n",
    "np.save('../models/rnn_product/data/index_in_order_history.npy', index_in_order_history)\n",
    "np.save('../models/rnn_product/data/order_dow_history.npy', order_dow_history)\n",
    "np.save('../models/rnn_product/data/order_hour_history.npy', order_hour_history)\n",
    "np.save('../models/rnn_product/data/days_since_prior_order_history.npy', days_since_prior_order_history)\n",
    "np.save('../models/rnn_product/data/order_size_history.npy', order_size_history)\n",
    "np.save('../models/rnn_product/data/reorder_size_history.npy', reorder_size_history)\n",
    "np.save('../models/rnn_product/data/order_number_history.npy', order_number_history)\n",
    "np.save('../models/rnn_product/data/product_name.npy', product_name)\n",
    "np.save('../models/rnn_product/data/product_name_length.npy', product_name_length)\n",
    "np.save('../models/rnn_product/data/history_length.npy', history_length)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Personalized Function\n",
    "sys.path.append(os.path.join(os.getcwd(), '../models'))\n",
    "from data_frame import DataFrame\n",
    "from tf_utils import lstm_layer, time_distributed_dense_layer, dense_layer, sequence_log_loss, wavenet\n",
    "from tf_base_model import TFBaseModel\n",
    "\n",
    "# Additional packages for python 2 functions\n",
    "from importlib import reload\n",
    "\n",
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        print(self.test_df.shapes())\n",
    "        print(\"loaded data\")\n",
    "\n",
    "        # Split the data into training and validation sets\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "        # Output set information\n",
    "        print('train size', len(self.train_df))\n",
    "        print('validation size', len(self.val_df))\n",
    "        print('test size', len(self.test_df))\n",
    "        \n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=10000,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            batch['is_none'] = batch['product_id'] == 0\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch\n",
    "            \n",
    "\n",
    "class rnn(TFBaseModel):\n",
    "\n",
    "    def __init__(self, lstm_size, dilations, filter_widths, skip_channels, residual_channels, **kwargs):\n",
    "        self.lstm_size = lstm_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        super(rnn, self).__init__(**kwargs)\n",
    "\n",
    "    def calculate_loss(self):\n",
    "        x = self.get_input_sequences()\n",
    "        preds = self.calculate_outputs(x)\n",
    "        loss = sequence_log_loss(self.next_is_ordered, preds, self.history_length, 100)\n",
    "        return loss\n",
    "\n",
    "    def get_input_sequences(self):\n",
    "        self.user_id = tf.placeholder(tf.int32, [None])\n",
    "        self.product_id = tf.placeholder(tf.int32, [None])\n",
    "        self.aisle_id = tf.placeholder(tf.int32, [None])\n",
    "        self.department_id = tf.placeholder(tf.int32, [None])\n",
    "        self.is_none = tf.placeholder(tf.int32, [None])\n",
    "        self.history_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.is_ordered_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.index_in_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_dow_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_hour_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.days_since_prior_order_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.reorder_size_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.order_number_history = tf.placeholder(tf.int32, [None, 100])\n",
    "        self.product_name = tf.placeholder(tf.int32, [None, 30])\n",
    "        self.product_name_length = tf.placeholder(tf.int32, [None])\n",
    "        self.next_is_ordered = tf.placeholder(tf.int32, [None, 100])\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # product data\n",
    "        product_embeddings = tf.get_variable(\n",
    "            name='product_embeddings',\n",
    "            shape=[50000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        aisle_embeddings = tf.get_variable(\n",
    "            name='aisle_embeddings',\n",
    "            shape=[250, 50],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        department_embeddings = tf.get_variable(\n",
    "            name='department_embeddings',\n",
    "            shape=[50, 10],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        product_names = tf.one_hot(self.product_name, 2532)\n",
    "        product_names = tf.reduce_max(product_names, 1)\n",
    "        product_names = dense_layer(product_names, 100, activation=tf.nn.relu)\n",
    "\n",
    "        is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "\n",
    "        x_product = tf.concat([\n",
    "            tf.nn.embedding_lookup(product_embeddings, self.product_id),\n",
    "            tf.nn.embedding_lookup(aisle_embeddings, self.aisle_id),\n",
    "            tf.nn.embedding_lookup(department_embeddings, self.department_id),\n",
    "            is_none,\n",
    "            product_names\n",
    "        ], axis=1)\n",
    "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "\n",
    "        # user data\n",
    "        user_embeddings = tf.get_variable(\n",
    "            name='user_embeddings',\n",
    "            shape=[207000, self.lstm_size],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        x_user = tf.nn.embedding_lookup(user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "\n",
    "        # sequence data\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history,\n",
    "            index_in_order_history,\n",
    "            order_dow_history,\n",
    "            order_hour_history,\n",
    "            days_since_prior_order_history,\n",
    "            order_size_history,\n",
    "            reorder_size_history,\n",
    "            order_number_history,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            reorder_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "        ], axis=2)\n",
    "\n",
    "        x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def calculate_outputs(self, x):\n",
    "        h = lstm_layer(x, self.history_length, self.lstm_size)\n",
    "        c = wavenet(x, self.dilations, self.filter_widths, self.skip_channels, self.residual_channels)\n",
    "        h = tf.concat([h, c, x], axis=2)\n",
    "\n",
    "        self.h_final = time_distributed_dense_layer(h, 50, activation=tf.nn.relu, scope='dense-1')\n",
    "        y_hat = time_distributed_dense_layer(self.h_final, 1, activation=tf.nn.sigmoid, scope='dense-2')\n",
    "        y_hat = tf.squeeze(y_hat, 2)\n",
    "\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), tf.maximum(self.history_length - 1, 0)], axis=1)\n",
    "        self.final_states = tf.gather_nd(self.h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "        }\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, '../models/rnn_product/data'))\n",
    "\n",
    "nn = rnn(\n",
    "    reader=dr,\n",
    "    log_dir=os.path.join(base_dir, 'logs'),\n",
    "    checkpoint_dir=os.path.join(base_dir, 'checkpoints'),\n",
    "    prediction_dir=os.path.join(base_dir, 'predictions'),\n",
    "    optimizer='adam',\n",
    "    learning_rate=.001,\n",
    "    lstm_size=300,\n",
    "    dilations=[2**i for i in range(6)],\n",
    "    filter_widths=[2]*6,\n",
    "    skip_channels=64,\n",
    "    residual_channels=128,\n",
    "    batch_size=128,\n",
    "    num_training_steps=1000,\n",
    "    early_stopping_steps=100,\n",
    "    warm_start_init_step=0,\n",
    "    regularization_constant=0.0,\n",
    "    keep_prob=1.0,\n",
    "    enable_parameter_averaging=False,\n",
    "    num_restarts=2,\n",
    "    min_steps_to_checkpoint=100,\n",
    "    log_interval=20,\n",
    "    num_validation_batches=4,\n",
    ")\n",
    "nn.fit() # Training finished, start prediction\n",
    "nn.restore()\n",
    "nn.predict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d334209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76588c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dccf06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd5a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7892d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383a239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
