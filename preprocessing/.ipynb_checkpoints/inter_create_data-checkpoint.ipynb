{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39106823",
   "metadata": {},
   "source": [
    "## User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f70884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output # Count iteration times\n",
    "# from datetime import datetime # Record time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "866a9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_order(x):\n",
    "    series = pd.Series(dtype='object') ### specify dtype\n",
    "\n",
    "    series['products'] = '_'.join(x['product_id'].values.astype(str).tolist())\n",
    "    series['reorders'] = '_'.join(x['reordered'].values.astype(str).tolist())\n",
    "    series['aisles'] = '_'.join(x['aisle_id'].values.astype(str).tolist())\n",
    "    series['departments'] = '_'.join(x['department_id'].values.astype(str).tolist())\n",
    "\n",
    "    series['order_number'] = x['order_number'].iloc[0]\n",
    "    series['order_dow'] = x['order_dow'].iloc[0]\n",
    "    series['order_hour'] = x['order_hour_of_day'].iloc[0]\n",
    "    series['days_since_prior_order'] = x['days_since_prior_order'].iloc[0]\n",
    "    \n",
    "    # Increment the counter and print the current count\n",
    "    global global_counter \n",
    "    global_counter += 1\n",
    "    clear_output(wait=True)\n",
    "    print(f\"total loops run: {global_counter}\")\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5cf04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_user(x):\n",
    "    parsed_orders = x.groupby('order_id', sort=False).apply(parse_order)\n",
    "\n",
    "    series = pd.Series(dtype='object')\n",
    "\n",
    "    series['order_ids'] = ' '.join(parsed_orders.index.map(str).tolist())\n",
    "    series['order_numbers'] = ' '.join(parsed_orders['order_number'].map(str).tolist())\n",
    "    series['order_dows'] = ' '.join(parsed_orders['order_dow'].map(str).tolist())\n",
    "    series['order_hours'] = ' '.join(parsed_orders['order_hour'].map(str).tolist())\n",
    "    series['days_since_prior_orders'] = ' '.join(parsed_orders['days_since_prior_order'].map(str).tolist())\n",
    "\n",
    "    series['product_ids'] = ' '.join(parsed_orders['products'].values.astype(str).tolist())\n",
    "    series['aisle_ids'] = ' '.join(parsed_orders['aisles'].values.astype(str).tolist())\n",
    "    series['department_ids'] = ' '.join(parsed_orders['departments'].values.astype(str).tolist())\n",
    "    series['reorders'] = ' '.join(parsed_orders['reorders'].values.astype(str).tolist())\n",
    "\n",
    "    series['eval_set'] = x['eval_set'].values[-1]\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a07dd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/storage/work/z/zbh5185/Instacart_Market/preprocessing'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() # Check the current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f5c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv('../data/raw/orders.csv')\n",
    "prior_products = pd.read_csv('../data/raw/order_products__prior.csv')\n",
    "train_products = pd.read_csv('../data/raw/order_products__train.csv')\n",
    "order_products = pd.concat([prior_products, train_products], axis=0)\n",
    "products = pd.read_csv('../data/raw/products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "333c438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orders.merge(order_products, how='left', on='order_id')\n",
    "df = df.merge(products, how='left', on='product_id')\n",
    "df['days_since_prior_order'] = df['days_since_prior_order'].fillna(0).astype(int)\n",
    "null_cols = ['product_id', 'aisle_id', 'department_id', 'add_to_cart_order', 'reordered']\n",
    "df[null_cols] = df[null_cols].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e0a004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_id', 'user_id', 'eval_set', 'order_number', 'order_dow',\n",
      "       'order_hour_of_day', 'days_since_prior_order', 'product_id',\n",
      "       'add_to_cart_order', 'reordered', 'product_name', 'aisle_id',\n",
      "       'department_id'],\n",
      "      dtype='object')\n",
      "(32434489, 4)\n",
      "(1384617, 4)\n",
      "['prior' 'train' 'test']\n"
     ]
    }
   ],
   "source": [
    "# DataFrame Check\n",
    "print(df.columns)\n",
    "print(prior_products.shape)\n",
    "print(train_products.shape)\n",
    "print(df['eval_set'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b610639",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('../data/processed'):\n",
    "    os.makedirs('../data/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8be49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Goal: sample 1% from df (by user), with each aisle and department are sampled at least tenth.\n",
    "\n",
    "# Identify all unique aisles and departments\n",
    "unique_aisles = df['aisle_id'].unique()\n",
    "unique_departments = df['department_id'].unique()\n",
    "\n",
    "# Initialize an empty dataframe to store the sampled data\n",
    "sampled_df = pd.DataFrame()\n",
    "\n",
    "# Sample at least ten rows for each aisle and department\n",
    "for aisle in unique_aisles:\n",
    "    sampled_df = sampled_df.append(df[df['aisle_id'] == aisle].sample(n=10))\n",
    "\n",
    "for department in unique_departments:\n",
    "    sampled_df = sampled_df.append(df[df['department_id'] == department].sample(n=10))\n",
    "\n",
    "# Identify unique users\n",
    "unique_users = df['user_id'].unique()\n",
    "\n",
    "# Calculate the number of users to sample\n",
    "num_users_to_sample = int(0.01 * len(unique_users))\n",
    "\n",
    "# Identify training users\n",
    "last_order_eval_set = df.groupby('user_id')['eval_set'].last()\n",
    "training_users = last_order_eval_set[last_order_eval_set == 'train'].index\n",
    "\n",
    "# Calculate the number of training and non-training users to sample\n",
    "num_training_users_to_sample = int(0.8 * num_users_to_sample)\n",
    "num_non_training_users_to_sample = num_users_to_sample - num_training_users_to_sample\n",
    "\n",
    "# Sample users\n",
    "sampled_training_users = np.random.choice(training_users, size=num_training_users_to_sample, replace=False)\n",
    "remaining_users = np.setdiff1d(unique_users, sampled_training_users)\n",
    "sampled_non_training_users = np.random.choice(remaining_users, size=num_non_training_users_to_sample, replace=False)\n",
    "\n",
    "# Reset the index of the sampled dataframe\n",
    "sampled_df = sampled_df.reset_index(drop=True)\n",
    "\n",
    "# Append the sampled users' data to the sampled dataframe\n",
    "sampled_df = pd.concat([sampled_df, df[df['user_id'].isin(sampled_training_users)], df[df['user_id'].isin(sampled_non_training_users)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cb429b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Add additional 1% for more informaiton\n",
    "\n",
    "# Identify the remaining data after the first round of sampling\n",
    "remaining_df = df.drop(sampled_df.index)\n",
    "\n",
    "# Calculate the number of rows to sample to get an additional 1% of the data\n",
    "additional_rows = int(0.01 * len(remaining_df))\n",
    "\n",
    "# Sample the additional rows randomly from the remaining data\n",
    "additional_sampled_df = remaining_df.sample(n=additional_rows)\n",
    "\n",
    "# Append the additional sampled rows to the sampled dataframe\n",
    "sampled_df = pd.concat([sampled_df, additional_sampled_df])\n",
    "\n",
    "# Reset the index of the sampled dataframe\n",
    "sampled_df = sampled_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be2c8bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "22\n",
      "(680748, 13)\n",
      "['prior' 'train']\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_aisles)) # aisle number check\n",
    "print(len(unique_departments)) # department number check\n",
    "print(sampled_df.shape) # Training Data Sampling Size\n",
    "print(df[df['user_id'].isin(sampled_training_users)]['eval_set'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06ecac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loops run: 1334451\n"
     ]
    }
   ],
   "source": [
    "# user_data = df.groupby('user_id', sort=False).apply(parse_user).reset_index()\n",
    "# Initialize a global counter\n",
    "global_counter = 0\n",
    "# user_data = sampled_df.groupby('user_id', sort=False).apply(parse_user).reset_index()\n",
    "\n",
    "### Full dataset version\n",
    "user_data = df.groupby('user_id', sort=False).apply(parse_user).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96e61eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.to_csv('../data/processed/user_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2f010",
   "metadata": {},
   "source": [
    "##  Product_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/user_data.csv')\n",
    "\n",
    "products = pd.read_csv('../data/raw/products.csv')\n",
    "product_to_aisle = dict(zip(products['product_id'], products['aisle_id']))\n",
    "product_to_department = dict(zip(products['product_id'], products['department_id']))\n",
    "product_to_name = dict(zip(products['product_id'], products['product_name']))\n",
    "\n",
    "user_ids = []\n",
    "product_ids = []\n",
    "aisle_ids = []\n",
    "department_ids = []\n",
    "product_names = []\n",
    "eval_sets = []\n",
    "\n",
    "is_ordered_histories = []\n",
    "index_in_order_histories = []\n",
    "order_size_histories = []\n",
    "reorder_size_histories = []\n",
    "order_dow_histories = []\n",
    "order_hour_histories = []\n",
    "days_since_prior_order_histories = []\n",
    "order_number_histories = []\n",
    "\n",
    "labels = []\n",
    "\n",
    "longest = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest = 0\n",
    "for _, row in df.iterrows():\n",
    "    if _ % 10000 == 0:\n",
    "        print(_)\n",
    "        data = [\n",
    "        user_ids,\n",
    "        product_ids,\n",
    "        aisle_ids,\n",
    "        department_ids,\n",
    "        product_names,\n",
    "        is_ordered_histories,\n",
    "        index_in_order_histories,\n",
    "        order_size_histories,\n",
    "        reorder_size_histories,\n",
    "        order_dow_histories,\n",
    "        order_hour_histories,\n",
    "        days_since_prior_order_histories,\n",
    "        order_number_histories,\n",
    "        labels,\n",
    "        eval_sets\n",
    "        ]\n",
    "        # Length Check\n",
    "        print(list(map(len, data)))\n",
    "\n",
    "    user_id = row['user_id']\n",
    "    eval_set = row['eval_set']\n",
    "    products = row['product_ids']\n",
    "\n",
    "    products, next_products = ' '.join(products.split()[:-1]), products.split()[-1]\n",
    "\n",
    "    reorders = row['reorders']\n",
    "    reorders, next_reorders = ' '.join(reorders.split()[:-1]), reorders.split()[-1]\n",
    "\n",
    "    product_set = set([int(j) for i in products.split() for j in i.split('_')])\n",
    "    next_product_set = set([int(i) for i in next_products.split('_')])\n",
    "\n",
    "    orders = [map(int, i.split('_')) for i in products.split()]\n",
    "    reorders = [map(int, i.split('_')) for i in reorders.split()]\n",
    "    next_reorders = map(int, next_reorders.split('_'))\n",
    "\n",
    "    for product_id in product_set:\n",
    "\n",
    "        user_ids.append(user_id)\n",
    "        product_ids.append(product_id)\n",
    "        labels.append(int(product_id in next_product_set) if eval_set == 'train' else -1)\n",
    "        eval_sets.append(eval_set) # Newly added\n",
    "\n",
    "        ### Handle Null\n",
    "        if product_id in product_to_aisle:\n",
    "            aisle_ids.append(product_to_aisle[product_id])\n",
    "        else:\n",
    "            aisle_ids.append('0')  # or some other default value\n",
    "\n",
    "        if product_id in product_to_department:\n",
    "            department_ids.append(product_to_department[product_id])\n",
    "        else:\n",
    "            department_ids.append('0')  # or some other default value\n",
    "\n",
    "        if product_id in product_to_name:\n",
    "            product_names.append(product_to_name[product_id])\n",
    "        else:\n",
    "            product_names.append('0')  # or some other default value\n",
    "\n",
    "        is_ordered = []\n",
    "        index_in_order = []\n",
    "        order_size = []\n",
    "        reorder_size = []\n",
    "\n",
    "        prior_products = set()\n",
    "        for order in orders:\n",
    "            is_ordered.append(str(int(product_id in order)))\n",
    "            order_list = list(order)\n",
    "            index_in_order.append(str(order_list.index(product_id) + 1) if product_id in order_list else '0')\n",
    "            order_size.append(str(len(list(order))))\n",
    "            reorder_size.append(str(len(list(prior_products & set(order)))))\n",
    "            prior_products |= set(order)\n",
    "\n",
    "        is_ordered = ' '.join(is_ordered)\n",
    "        index_in_order = ' '.join(index_in_order)\n",
    "        order_size = ' '.join(order_size)\n",
    "        reorder_size = ' '.join(reorder_size)\n",
    "\n",
    "        is_ordered_histories.append(is_ordered)\n",
    "        index_in_order_histories.append(index_in_order)\n",
    "        order_size_histories.append(order_size)\n",
    "        reorder_size_histories.append(reorder_size)\n",
    "        order_dow_histories.append(row['order_dows'])\n",
    "        order_hour_histories.append(row['order_hours'])\n",
    "        days_since_prior_order_histories.append(row['days_since_prior_orders'])\n",
    "        order_number_histories.append(row['order_numbers'])\n",
    "\n",
    "    user_ids.append(user_id)\n",
    "    product_ids.append(0)\n",
    "    # labels.append(int(max(next_reorders) == 0) if eval_set == 'train' else -1)\n",
    "    # Do not consider the case \"no order\"\n",
    "    labels.append(-1)\n",
    "\n",
    "    aisle_ids.append(0)\n",
    "    department_ids.append(0)\n",
    "    product_names.append(0)\n",
    "    eval_sets.append(eval_set)\n",
    "\n",
    "    is_ordered = []\n",
    "    index_in_order = []\n",
    "    order_size = []\n",
    "    reorder_size = []\n",
    "\n",
    "    for reorder in reorders:\n",
    "        is_ordered.append(str(int(max(reorder) == 0)))\n",
    "        index_in_order.append(str(0))\n",
    "        order_size.append(str(len(list(reorder))))\n",
    "        reorder_size.append(str(sum(reorder)))\n",
    "\n",
    "    is_ordered = ' '.join(is_ordered)\n",
    "    index_in_order = ' '.join(index_in_order)\n",
    "    order_size = ' '.join(order_size)\n",
    "    reorder_size = ' '.join(reorder_size)\n",
    "\n",
    "    is_ordered_histories.append(is_ordered)\n",
    "    index_in_order_histories.append(index_in_order)\n",
    "    order_size_histories.append(order_size)\n",
    "    reorder_size_histories.append(reorder_size)\n",
    "    order_dow_histories.append(row['order_dows'])\n",
    "    order_hour_histories.append(row['order_hours'])\n",
    "    days_since_prior_order_histories.append(row['days_since_prior_orders'])\n",
    "    order_number_histories.append(row['order_numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "        user_ids,\n",
    "        product_ids,\n",
    "        aisle_ids,\n",
    "        department_ids,\n",
    "        product_names,\n",
    "        is_ordered_histories,\n",
    "        index_in_order_histories,\n",
    "        order_size_histories,\n",
    "        reorder_size_histories,\n",
    "        order_dow_histories,\n",
    "        order_hour_histories,\n",
    "        days_since_prior_order_histories,\n",
    "        order_number_histories,\n",
    "        labels,\n",
    "        eval_sets\n",
    "    ]\n",
    "# Length Check\n",
    "\n",
    "list(map(len, data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7694d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'user_id',\n",
    "    'product_id',\n",
    "    'aisle_id',\n",
    "    'department_id',\n",
    "    'product_name',\n",
    "    'is_ordered_history',\n",
    "    'index_in_order_history',\n",
    "    'order_size_history',\n",
    "    'reorder_size_history',\n",
    "    'order_dow_history',\n",
    "    'order_hour_history',\n",
    "    'days_since_prior_order_history',\n",
    "    'order_number_history',\n",
    "    'label',\n",
    "    'eval_set'\n",
    "]\n",
    "if not os.path.isdir('../data/processed'):\n",
    "    os.makedirs('../data/processed')\n",
    "\n",
    "df = pd.DataFrame(dict(zip(columns, data)))\n",
    "df.to_csv('../data/processed/product_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa7964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
